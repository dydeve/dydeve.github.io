<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[向 clickhouse-jdbc 学习 jdbc]]></title>
    <url>%2F2019%2F05%2F27%2Fclickhouse-jdbc%2F</url>
    <content type="text"><![CDATA[clickhouse-jdbc是clickhouse的jdbc驱动，本文从clickhouse-jdbc探索jdbc的一般实现 ClickHouseDriver12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788/** * * URL Format * * primitive for now * * jdbc:clickhouse://host:port * * for example, jdbc:clickhouse://localhost:8123 * */public class ClickHouseDriver implements Driver &#123; private static final Logger logger = LoggerFactory.getLogger(ClickHouseDriver.class); private static final ConcurrentMap&lt;ClickHouseConnectionImpl, Boolean&gt; connections = new MapMaker().weakKeys().makeMap(); static &#123; ClickHouseDriver driver = new ClickHouseDriver(); try &#123; DriverManager.registerDriver(driver);//注册驱动 &#125; catch (SQLException e) &#123; throw new RuntimeException(e); &#125; logger.info("Driver registered"); &#125; @Override public ClickHouseConnection connect(String url, Properties info) throws SQLException &#123; return connect(url, new ClickHouseProperties(info)); &#125; public ClickHouseConnection connect(String url, ClickHouseProperties properties) throws SQLException &#123; if (!acceptsURL(url)) &#123; return null; &#125; logger.debug("Creating connection"); ClickHouseConnectionImpl connection = new ClickHouseConnectionImpl(url, properties); registerConnection(connection); return LogProxy.wrap(ClickHouseConnection.class, connection); &#125; private void registerConnection(ClickHouseConnectionImpl connection) &#123; connections.put(connection, Boolean.TRUE); &#125; @Override public boolean acceptsURL(String url) throws SQLException &#123; return url.startsWith(ClickhouseJdbcUrlParser.JDBC_CLICKHOUSE_PREFIX); &#125; //... /** * Schedules connections cleaning at a rate. Turned off by default. * See https://hc.apache.org/httpcomponents-client-4.5.x/tutorial/html/connmgmt.html#d5e418 * 定时清理连接，本质上清理httpclient * @param rate * @param timeUnit */ public void scheduleConnectionsCleaning(int rate, TimeUnit timeUnit)&#123; ScheduledConnectionCleaner.INSTANCE.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; try &#123; for (ClickHouseConnectionImpl connection : connections.keySet()) &#123; connection.cleanConnections(); &#125; &#125; catch (Exception e)&#123; logger.error("error evicting connections: " + e); &#125; &#125; &#125;, 0, rate, timeUnit); &#125; static class ScheduledConnectionCleaner &#123; static final ScheduledExecutorService INSTANCE = Executors.newSingleThreadScheduledExecutor(new DaemonThreadFactory()); static class DaemonThreadFactory implements ThreadFactory &#123; @Override public Thread newThread(Runnable r) &#123; Thread thread = Executors.defaultThreadFactory().newThread(r); thread.setDaemon(true); return thread; &#125; &#125; &#125;&#125; 主要是构建ClickHouseConnectionImpl，实现Driver#connect(String url, java.util.Properties info) throws SQLException 方法 ClickHouseConnectionImpl12345678910111213141516public interface ClickHouseConnection extends Connection &#123; @Deprecated ClickHouseStatement createClickHouseStatement() throws SQLException; TimeZone getTimeZone(); // 将返回值类型 Statement 改为 ClickHouseStatement，更灵活 @Override ClickHouseStatement createStatement() throws SQLException; @Override ClickHouseStatement createStatement(int resultSetType, int resultSetConcurrency) throws SQLException; String getServerVersion() throws SQLException;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class ClickHouseConnectionImpl implements ClickHouseConnection &#123; private static final int DEFAULT_RESULTSET_TYPE = ResultSet.TYPE_FORWARD_ONLY; private static final Logger log = LoggerFactory.getLogger(ClickHouseConnectionImpl.class); private final CloseableHttpClient httpclient; private final ClickHouseProperties properties; private String url; private boolean closed = false; private TimeZone timezone; private volatile String serverVersion; public ClickHouseConnectionImpl(String url) &#123; this(url, new ClickHouseProperties()); &#125; // 可以看到，ClickHouseConnectionImpl 本质上是 httpclient public ClickHouseConnectionImpl(String url, ClickHouseProperties properties) &#123; this.url = url; try &#123; this.properties = ClickhouseJdbcUrlParser.parse(url, properties.asProperties()); &#125; catch (URISyntaxException e) &#123; throw new IllegalArgumentException(e); &#125; ClickHouseHttpClientBuilder clientBuilder = new ClickHouseHttpClientBuilder(this.properties); log.debug("new connection"); try &#123; httpclient = clientBuilder.buildClient(); &#125;catch (Exception e) &#123; throw new IllegalStateException("cannot initialize http client", e); &#125; initTimeZone(this.properties); &#125; public ClickHouseStatement createStatement(int resultSetType) throws SQLException &#123; return LogProxy.wrap(ClickHouseStatement.class, new ClickHouseStatementImpl(httpclient, this, properties, resultSetType)); &#125; void cleanConnections() &#123; httpclient.getConnectionManager().closeExpiredConnections(); httpclient.getConnectionManager().closeIdleConnections(2 * properties.getSocketTimeout(), TimeUnit.MILLISECONDS); &#125; // 主要作用是创建 ClickHouseStatement&#125; ClickHouseStatementImpl1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class ClickHouseStatementImpl implements ClickHouseStatement &#123; @Override public ResultSet executeQuery(String sql, Map&lt;ClickHouseQueryParam, String&gt; additionalDBParams, List&lt;ClickHouseExternalData&gt; externalData, Map&lt;String, String&gt; additionalRequestParams) throws SQLException &#123; // forcibly disable extremes for ResultSet queries if (additionalDBParams == null || additionalDBParams.isEmpty()) &#123; additionalDBParams = new EnumMap&lt;ClickHouseQueryParam, String&gt;(ClickHouseQueryParam.class); &#125; else &#123; additionalDBParams = new EnumMap&lt;ClickHouseQueryParam, String&gt;(additionalDBParams); &#125; additionalDBParams.put(ClickHouseQueryParam.EXTREMES, "0"); InputStream is = getInputStream(sql, additionalDBParams, externalData, additionalRequestParams); try &#123; if (isSelect(sql)) &#123; currentUpdateCount = -1; currentResult = createResultSet(properties.isCompress() ? new ClickHouseLZ4Stream(is) : is, properties.getBufferSize(), extractDBName(sql), extractTableName(sql), extractWithTotals(sql), this, getConnection().getTimeZone(), properties ); currentResult.setMaxRows(maxRows); return currentResult; &#125; else &#123; currentUpdateCount = 0; StreamUtils.close(is); return null; &#125; &#125; catch (Exception e) &#123; StreamUtils.close(is); throw ClickHouseExceptionSpecifier.specify(e, properties.getHost(), properties.getPort()); &#125; &#125; private ClickHouseResultSet createResultSet(InputStream is, int bufferSize, String db, String table, boolean usesWithTotals, ClickHouseStatement statement, TimeZone timezone, ClickHouseProperties properties) throws IOException &#123; if(isResultSetScrollable) &#123; return new ClickHouseScrollableResultSet(is, bufferSize, db, table, usesWithTotals, statement, timezone, properties); &#125; else &#123; return new ClickHouseResultSet(is, bufferSize, db, table, usesWithTotals, statement, timezone, properties); &#125; &#125;&#125; executeQuery方法将sql语句组成参数，通过httpclient传给server，返回stream(会用到FastByteArrayOutputStream[去除synchronized的ByteArrayOutputStream])。然后把stream封装到ClickHouseResultSet BalancedClickhouseDataSource当调用getConnection方法，会返回连接到随机主机的connection.也可以周期检查连接是否活跃。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110/** * &lt;p&gt; Database for clickhouse jdbc connections. * &lt;p&gt; It has list of database urls. * For every &#123;@link #getConnection() getConnection&#125; invocation, it returns connection to random host from the list. * Furthermore, this class has method &#123;@link #scheduleActualization(int, TimeUnit) scheduleActualization&#125; * which test hosts for availability. By default, this option is turned off. */public class BalancedClickhouseDataSource implements DataSource &#123; private static final org.slf4j.Logger log = LoggerFactory.getLogger(BalancedClickhouseDataSource.class); private static final Pattern URL_TEMPLATE = Pattern.compile(JDBC_CLICKHOUSE_PREFIX + "" + "//([a-zA-Z0-9_:,.-]+)" + "(/[a-zA-Z0-9_]+" + "([?][a-zA-Z0-9_]+[=][a-zA-Z0-9_]+([&amp;][a-zA-Z0-9_]+[=][a-zA-Z0-9_]+)*)?" + ")?"); private PrintWriter printWriter; private int loginTimeoutSeconds = 0; private final ThreadLocal&lt;Random&gt; randomThreadLocal = new ThreadLocal&lt;Random&gt;(); private final List&lt;String&gt; allUrls; private volatile List&lt;String&gt; enabledUrls; private final ClickHouseProperties properties; private final ClickHouseDriver driver = new ClickHouseDriver(); private boolean ping(final String url) &#123; try &#123; driver.connect(url, properties).createStatement().execute("SELECT 1"); return true; &#125; catch (Exception e) &#123; return false; &#125; &#125; /** * Checks if clickhouse on url is alive, if it isn't, disable url, else enable. * * @return number of avaliable clickhouse urls */ public synchronized int actualize() &#123; List&lt;String&gt; enabledUrls = new ArrayList&lt;String&gt;(allUrls.size()); for (String url : allUrls) &#123; log.debug("Pinging disabled url: &#123;&#125;", url); if (ping(url)) &#123; log.debug("Url is alive now: &#123;&#125;", url); enabledUrls.add(url); &#125; else &#123; log.debug("Url is dead now: &#123;&#125;", url); &#125; &#125; this.enabledUrls = Collections.unmodifiableList(enabledUrls); return enabledUrls.size(); &#125; private String getAnyUrl() throws SQLException &#123; List&lt;String&gt; localEnabledUrls = enabledUrls; if (localEnabledUrls.isEmpty()) &#123; throw new SQLException("Unable to get connection: there are no enabled urls"); &#125; Random random = this.randomThreadLocal.get(); if (random == null) &#123; this.randomThreadLocal.set(new Random(System.currentTimeMillis())); random = this.randomThreadLocal.get(); &#125; int index = random.nextInt(localEnabledUrls.size());//随机选择链接 return localEnabledUrls.get(index); &#125; /** * &#123;@inheritDoc&#125; */ @Override public ClickHouseConnection getConnection() throws SQLException &#123; return driver.connect(getAnyUrl(), properties); &#125; /** * &#123;@inheritDoc&#125; */ @Override public ClickHouseConnection getConnection(String username, String password) throws SQLException &#123; return driver.connect(getAnyUrl(), properties.withCredentials(username, password)); &#125; /** * set time period for checking availability connections * 周期检查可用性 * @param delay value for time unit * @param timeUnit time unit for checking * @return this datasource with changed settings */ public BalancedClickhouseDataSource scheduleActualization(int delay, TimeUnit timeUnit) &#123; ClickHouseDriver.ScheduledConnectionCleaner.INSTANCE.scheduleWithFixedDelay(new Runnable() &#123; @Override public void run() &#123; try &#123; actualize(); &#125; catch (Exception e) &#123; log.error("Unable to actualize urls", e); &#125; &#125; &#125;, 0, delay, timeUnit); return this; &#125;&#125; LogProxyLogProxy在很多地方用到，主要功能是，当log可trace时，打印sql的上下文123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class LogProxy&lt;T&gt; implements InvocationHandler &#123; private static final Logger log = LoggerFactory.getLogger(LogProxy.class); private final T object; private final Class&lt;T&gt; clazz; public static &lt;T&gt; T wrap(Class&lt;T&gt; interfaceClass, T object) &#123; if (log.isTraceEnabled()) &#123;//仅当日志级别为trace，返回代理对象 LogProxy&lt;T&gt; proxy = new LogProxy&lt;T&gt;(interfaceClass, object); return proxy.getProxy(); &#125; return object; &#125; private LogProxy(Class&lt;T&gt; interfaceClass, T object) &#123; if (!interfaceClass.isInterface()) &#123; throw new IllegalStateException("Class " + interfaceClass.getName() + " is not an interface"); &#125; clazz = interfaceClass; this.object = object; &#125; @SuppressWarnings("unchecked") public T getProxy() &#123; //xnoinspection x // unchecked return (T) Proxy.newProxyInstance(clazz.getClassLoader(), new Class&lt;?&gt;[]&#123;clazz&#125;, this); &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; String msg = "Call class: " + object.getClass().getName() + "\nMethod: " + method.getName() + "\nObject: " + object + "\nArgs: " + Arrays.toString(args) + "\nInvoke result: "; try &#123; final Object invokeResult = method.invoke(object, args); msg += invokeResult; return invokeResult; &#125; catch (InvocationTargetException e) &#123; msg += e.getMessage(); throw e.getTargetException(); &#125; finally &#123; msg = "==== ClickHouse JDBC trace begin ====\n" + msg + "\n==== ClickHouse JDBC trace end ===="; log.trace(msg); &#125; &#125;&#125;]]></content>
      <categories>
        <category>util</category>
      </categories>
      <tags>
        <tag>clickhouse</tag>
        <tag>jdbc</tag>
        <tag>httpclient</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[metrics度量]]></title>
    <url>%2F2019%2F05%2F23%2Fmetrics%2F</url>
    <content type="text"><![CDATA[简介metrics让你以非并行的视角看到你的代码做了什么，给你强大的工具箱衡量生产环境中关键组件的行为。 使用通用库，例如Jetty, Logback, Log4j, Apache HttpClient, Ehcache, JDBI, Jersey，报告给Graphite，metrics给你全栈的可视化。 Metrics Core mertic 注册 5 metric types: Gauges, Counters, Histograms, Meters, and Timers. 报告度量值，via JMX,console,log,csv Metric Registries度量注册的主类是MetricRegistry,所有metric实例的注册服务。通常，一个应用一个MetricRegistry，(spark每个source一个registry，如DAGSchedulerSource) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150public class MetricRegistry implements MetricSet &#123; // 以.相连 public static String name(String name, String... names) &#123; final StringBuilder builder = new StringBuilder(); append(builder, name); if (names != null) &#123; for (String s : names) &#123; append(builder, s); &#125; &#125; return builder.toString(); &#125; public static String name(Class&lt;?&gt; klass, String... names) &#123; return name(klass.getName(), names); &#125; private static void append(StringBuilder builder, String part) &#123; if (part != null &amp;&amp; !part.isEmpty()) &#123; if (builder.length() &gt; 0) &#123; builder.append('.'); &#125; builder.append(part); &#125; &#125; private final ConcurrentMap&lt;String, Metric&gt; metrics; private final List&lt;MetricRegistryListener&gt; listeners; public MetricRegistry() &#123; this.metrics = buildMap(); this.listeners = new CopyOnWriteArrayList&lt;MetricRegistryListener&gt;(); &#125; /** * Given a &#123;@link Metric&#125;, registers it under the given name. * 以&lt;name, metric&gt;的形式，插入metrics * @param name the name of the metric * @param metric the metric * @param &lt;T&gt; the type of the metric * @return &#123;@code metric&#125; * @throws IllegalArgumentException if the name is already registered */ @SuppressWarnings("unchecked") public &lt;T extends Metric&gt; T register(String name, T metric) throws IllegalArgumentException &#123; if (metric instanceof MetricSet) &#123; registerAll(name, (MetricSet) metric); &#125; else &#123; final Metric existing = metrics.putIfAbsent(name, metric); if (existing == null) &#123; onMetricAdded(name, metric); &#125; else &#123; throw new IllegalArgumentException("A metric named " + name + " already exists"); &#125; &#125; return metric; &#125; /** * Given a metric set, registers them. * * @param metrics a set of metrics * @throws IllegalArgumentException if any of the names are already registered */ public void registerAll(MetricSet metrics) throws IllegalArgumentException &#123; registerAll(null, metrics); &#125; private void registerAll(String prefix, MetricSet metrics) throws IllegalArgumentException &#123; for (Map.Entry&lt;String, Metric&gt; entry : metrics.getMetrics().entrySet()) &#123; if (entry.getValue() instanceof MetricSet) &#123; registerAll(name(prefix, entry.getKey()), (MetricSet) entry.getValue()); &#125; else &#123; register(name(prefix, entry.getKey()), entry.getValue()); &#125; &#125; &#125; @Override public Map&lt;String, Metric&gt; getMetrics() &#123; return Collections.unmodifiableMap(metrics); &#125; /** * Return the &#123;@link Counter&#125; registered under this name; or create and register * a new &#123;@link Counter&#125; if none is registered. * * @param name the name of the metric * @return a new or pre-existing &#123;@link Counter&#125; */ public Counter counter(String name) &#123; return getOrAdd(name, MetricBuilder.COUNTERS); &#125; // histogram(String name) meter(String name) timer(String name) private &lt;T extends Metric&gt; T getOrAdd(String name, MetricBuilder&lt;T&gt; builder) &#123; final Metric metric = metrics.get(name); if (builder.isInstance(metric)) &#123; return (T) metric; &#125; else if (metric == null) &#123; try &#123; return register(name, builder.newMetric()); &#125; catch (IllegalArgumentException e) &#123; final Metric added = metrics.get(name); if (builder.isInstance(added)) &#123; return (T) added; &#125; &#125; &#125; throw new IllegalArgumentException(name + " is already used for a different type of metric"); &#125; /** * A quick and easy way of capturing the notion of default metrics. */ private interface MetricBuilder&lt;T extends Metric&gt; &#123; MetricBuilder&lt;Counter&gt; COUNTERS = new MetricBuilder&lt;Counter&gt;() &#123; @Override public Counter newMetric() &#123; return new Counter(); &#125; @Override public boolean isInstance(Metric metric) &#123; return Counter.class.isInstance(metric); &#125; &#125;; // METERS Timer MetricBuilder&lt;Histogram&gt; HISTOGRAMS = new MetricBuilder&lt;Histogram&gt;() &#123; @Override public Histogram newMetric() &#123; return new Histogram(new ExponentiallyDecayingReservoir()); &#125; @Override public boolean isInstance(Metric metric) &#123; return Histogram.class.isInstance(metric); &#125; &#125;; T newMetric(); boolean isInstance(Metric metric); &#125;&#125; 省略了监听器相关代码，因为比较简单，且不是重点。 5 metric types Gauge即时读取特定的值(an instantaneous reading of a particular value)12345678public interface Gauge&lt;T&gt; extends Metric &#123; /** * Returns the metric's current value. * * @return the metric's current value */ T getValue();&#125; example:123456final Queue&lt;String&gt; queue = new ConcurrentLinkedQueue&lt;String&gt;();final Gauge&lt;Integer&gt; queueDepth = new Gauge&lt;Integer&gt;() &#123; public Integer getValue() &#123; return queue.size(); &#125;&#125;; Gauge有几个实现类:RatioGauge,JmxAttributeGauge,DerivativeGauge以CachedGauge为例,可用于耗时的操作12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public abstract class CachedGauge&lt;T&gt; implements Gauge&lt;T&gt; &#123; private final Clock clock; private final AtomicLong reloadAt; private final long timeoutNS; private volatile T value;// 缓存 /** * Creates a new cached gauge with the given timeout period. * * @param timeout the timeout * @param timeoutUnit the unit of &#123;@code timeout&#125; */ protected CachedGauge(long timeout, TimeUnit timeoutUnit) &#123; this(Clock.defaultClock(), timeout, timeoutUnit); &#125; /** * Creates a new cached gauge with the given clock and timeout period. * * @param clock the clock used to calculate the timeout * @param timeout the timeout * @param timeoutUnit the unit of &#123;@code timeout&#125; */ protected CachedGauge(Clock clock, long timeout, TimeUnit timeoutUnit) &#123; this.clock = clock; this.reloadAt = new AtomicLong(0); this.timeoutNS = timeoutUnit.toNanos(timeout); &#125; /** * Loads the value and returns it. * 更新缓存 * @return the new value */ protected abstract T loadValue(); @Override public T getValue() &#123; if (shouldLoad()) &#123; this.value = loadValue(); &#125; return value; &#125; private boolean shouldLoad() &#123; for (; ; ) &#123; final long time = clock.getTick(); final long current = reloadAt.get(); if (current &gt; time) &#123; return false; &#125; if (reloadAt.compareAndSet(current, time + timeoutNS)) &#123; return true; &#125; &#125; &#125;&#125; Counter12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * An incrementing and decrementing counter metric. */public class Counter implements Metric, Counting &#123; private final LongAdderAdapter count; public Counter() &#123; this.count = LongAdderProxy.create(); &#125; /** * Increment the counter by one. */ public void inc() &#123; inc(1); &#125; /** * Increment the counter by &#123;@code n&#125;. * * @param n the amount by which the counter will be increased */ public void inc(long n) &#123; count.add(n); &#125; /** * Decrement the counter by one. */ public void dec() &#123; dec(1); &#125; /** * Decrement the counter by &#123;@code n&#125;. * * @param n the amount by which the counter will be decreased */ public void dec(long n) &#123; count.add(-n); &#125; /** * Returns the counter's current value. * * @return the counter's current value */ @Override public long getCount() &#123; return count.sum(); &#125;&#125; Histogram直方图,度量数据流的分布123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * A metric which calculates the distribution of a value. * * @see &lt;a href="http://www.johndcook.com/standard_deviation.html"&gt;Accurately computing running * variance&lt;/a&gt; */public class Histogram implements Metric, Sampling, Counting &#123; private final Reservoir reservoir; private final LongAdderAdapter count; /** * Creates a new &#123;@link Histogram&#125; with the given reservoir. * * @param reservoir the reservoir to create a histogram from */ public Histogram(Reservoir reservoir) &#123; this.reservoir = reservoir; this.count = LongAdderProxy.create(); &#125; /** * Adds a recorded value. * * @param value the length of the value */ public void update(int value) &#123; update((long) value); &#125; /** * Adds a recorded value. * * @param value the length of the value */ public void update(long value) &#123; count.increment(); reservoir.update(value); &#125; /** * Returns the number of values recorded. * * @return the number of values recorded */ @Override public long getCount() &#123; return count.sum(); &#125; @Override public Snapshot getSnapshot() &#123; return reservoir.getSnapshot(); &#125;&#125;public abstract class Snapshot &#123; public double getMedian() public double get75thPercentile() ...&#125; 同时具备Counter#getCount()功能 和 Reservoir#getSnapshot()方法 直方图可以测量min,max,mean,standard deviation，以及分位数，如中位数、p95等等 通常，取分位数需要获取全量数据集，排序，取值。这适合用于小数据集、批处理系统，不适用于高吞吐、低延迟服务。 解决方法，是在数据经过时取样。通过维护一个小的、可管理的、在统计上代表整个数据流的储层，我们可以快速而容易地计算出分位数，这是实际分位数的有效近似值。这种技术被称为储层取样(reservoir sampling)。 目前共有以下几种储层 Meter测量平均吞吐量，1min、5min、15min的指数权重吞吐量(rate * duration)1Just like the Unix load averages visible in uptime or top. Timers12345678910/** * A timer metric which aggregates timing durations and provides duration statistics, plus * throughput statistics via &#123;@link Meter&#125;. */public class Timer implements Metered, Sampling &#123; private final Meter meter; private final Histogram histogram; private final Clock clock;&#125; 聚合时长、时长统计(histogram)、以及吞吐量(meter) reporters导出metrics统计出的结果，metrics-core提供了四种方式：JMX, console, SLF4J, and CSV. ScheduledReporter123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172/** * The abstract base class for all scheduled reporters (i.e., reporters which process a registry's * metrics periodically). * * 调度reporters的基类，周期处理registry的metrics * @see ConsoleReporter * @see CsvReporter * @see Slf4jReporter */public abstract class ScheduledReporter implements Closeable, Reporter &#123; private static final Logger LOG = LoggerFactory.getLogger(ScheduledReporter.class); /** * A simple named thread factory. */ @SuppressWarnings("NullableProblems") private static class NamedThreadFactory implements ThreadFactory &#123; private final ThreadGroup group; private final AtomicInteger threadNumber = new AtomicInteger(1); private final String namePrefix; private NamedThreadFactory(String name) &#123; final SecurityManager s = System.getSecurityManager(); this.group = (s != null) ? s.getThreadGroup() : Thread.currentThread().getThreadGroup(); this.namePrefix = "metrics-" + name + "-thread-"; &#125; @Override public Thread newThread(Runnable r) &#123; final Thread t = new Thread(group, r, namePrefix + threadNumber.getAndIncrement(), 0); t.setDaemon(true); if (t.getPriority() != Thread.NORM_PRIORITY) &#123; t.setPriority(Thread.NORM_PRIORITY); &#125; return t; &#125; &#125; private static final AtomicInteger FACTORY_ID = new AtomicInteger(); private final MetricRegistry registry; private final ScheduledExecutorService executor; private final MetricFilter filter; private final double durationFactor; private final String durationUnit; private final double rateFactor; private final String rateUnit; /** * Creates a new &#123;@link ScheduledReporter&#125; instance. * * @param registry the &#123;@link com.codahale.metrics.MetricRegistry&#125; containing the metrics this * reporter will report * @param name the reporter's name * @param filter the filter for which metrics to report * @param rateUnit a unit of time * @param durationUnit a unit of time */ protected ScheduledReporter(MetricRegistry registry, String name, MetricFilter filter, TimeUnit rateUnit, TimeUnit durationUnit) &#123; this(registry, name, filter, rateUnit, durationUnit, Executors.newSingleThreadScheduledExecutor(new NamedThreadFactory(name + '-' + FACTORY_ID.incrementAndGet()))); &#125; /** * Creates a new &#123;@link ScheduledReporter&#125; instance. * * @param registry the &#123;@link com.codahale.metrics.MetricRegistry&#125; containing the metrics this * reporter will report * @param name the reporter's name * @param filter the filter for which metrics to report * @param executor the executor to use while scheduling reporting of metrics. */ protected ScheduledReporter(MetricRegistry registry, String name, MetricFilter filter, TimeUnit rateUnit, TimeUnit durationUnit, ScheduledExecutorService executor) &#123; this.registry = registry; this.filter = filter; this.executor = executor; this.rateFactor = rateUnit.toSeconds(1); this.rateUnit = calculateRateUnit(rateUnit); this.durationFactor = 1.0 / durationUnit.toNanos(1); this.durationUnit = durationUnit.toString().toLowerCase(Locale.US); &#125; /** * Starts the reporter polling at the given period. * 开始定时任务 * @param period the amount of time between polls * @param unit the unit for &#123;@code period&#125; */ public void start(long period, TimeUnit unit) &#123; executor.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; try &#123; report(); &#125; catch (Exception ex) &#123; LOG.error("Exception thrown from &#123;&#125;#report. Exception was suppressed.", ScheduledReporter.this.getClass().getSimpleName(), ex); &#125; &#125; &#125;, period, period, unit); &#125; /** * Stops the reporter and shuts down its thread of execution. * * Uses the shutdown pattern from http://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ExecutorService.html */ public void stop() &#123; executor.shutdown(); // Disable new tasks from being submitted try &#123; // Wait a while for existing tasks to terminate if (!executor.awaitTermination(1, TimeUnit.SECONDS)) &#123; executor.shutdownNow(); // Cancel currently executing tasks // Wait a while for tasks to respond to being cancelled if (!executor.awaitTermination(1, TimeUnit.SECONDS)) &#123; System.err.println(getClass().getSimpleName() + ": ScheduledExecutorService did not terminate"); &#125; &#125; &#125; catch (InterruptedException ie) &#123; // (Re-)Cancel if current thread also interrupted executor.shutdownNow(); // Preserve interrupt status Thread.currentThread().interrupt(); &#125; &#125; /** * Stops the reporter and shuts down its thread of execution. */ @Override public void close() &#123; stop(); &#125; /** * Report the current values of all metrics in the registry. * 输出metrics，synchronized */ public void report() &#123; synchronized (this) &#123; report(registry.getGauges(filter), registry.getCounters(filter), registry.getHistograms(filter), registry.getMeters(filter), registry.getTimers(filter)); &#125; &#125; /** * Called periodically by the polling thread. Subclasses should report all the given metrics. * 留给子类继承 * @param gauges all of the gauges in the registry * @param counters all of the counters in the registry * @param histograms all of the histograms in the registry * @param meters all of the meters in the registry * @param timers all of the timers in the registry */ public abstract void report(SortedMap&lt;String, Gauge&gt; gauges, SortedMap&lt;String, Counter&gt; counters, SortedMap&lt;String, Histogram&gt; histograms, SortedMap&lt;String, Meter&gt; meters, SortedMap&lt;String, Timer&gt; timers);&#125; 所谓的start、stop，就是调度定时器、关闭定时器 Other ReportersMetricsServlet 健康检查、thread dump、JVM-level and OS-level信息 GraphiteReporter图形界面 JVM Instrumentationmetrics-jvm - Run count and elapsed times for all supported garbage collectors - Memory usage for all memory pools, including off-heap memory - Breakdown of thread states, including deadlocks - File descriptor usage - Buffer pool sizes and utilization Monitoring your JVM with Dropwizard Metrics 参考metrics操作手册]]></content>
      <categories>
        <category>util</category>
      </categories>
      <tags>
        <tag>monitor</tag>
        <tag>metric</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark mertric度量]]></title>
    <url>%2F2019%2F05%2F23%2Fspark-mertric%2F</url>
    <content type="text"><![CDATA[概览度量，metric，及对系统运行过程中的状态进行收集统计，如性能、数据等，方便开发者了解状况，针对性优化等。 spark使用metrics进行监控 spark中，度量有3个概念： Instance:度量系统的实例名。Spark按照Instance的不同，分为Master、Worker、Application、Driver和Executor Source:数据来源，WorkerSource，DAGSchedulerSource，BlockManagerSource等 Sink:数据输出，MetricsServlet，ConsoleSink，Slf4jSink等 这些概念和flume类似 1234567891011121314151617181920212223242526272829303132333435363738394041/** * Spark Metrics System, created by a specific "instance", combined by source, * sink, periodically polls source metrics data to sink destinations. * * "instance" specifies "who" (the role) uses the metrics system. In Spark, there are several roles * like master, worker, executor, client driver. These roles will create metrics system * for monitoring. So, "instance" represents these roles. Currently in Spark, several instances * have already implemented: master, worker, executor, driver, applications. * * "source" specifies "where" (source) to collect metrics data from. In metrics system, there exists * two kinds of source: * 1. Spark internal source, like MasterSource, WorkerSource, etc, which will collect * Spark component's internal state, these sources are related to instance and will be * added after a specific metrics system is created. * 2. Common source, like JvmSource, which will collect low level state, is configured by * configuration and loaded through reflection. * * "sink" specifies "where" (destination) to output metrics data to. Several sinks can * coexist and metrics can be flushed to all these sinks. * * Metrics configuration format is like below: * [instance].[sink|source].[name].[options] = xxxx * * [instance] can be "master", "worker", "executor", "driver", "applications" which means only * the specified instance has this property. * wild card "*" can be used to replace instance name, which means all the instances will have * this property. * * [sink|source] means this property belongs to source or sink. This field can only be * source or sink. * * [name] specify the name of sink or source, if it is custom defined. * * [options] represent the specific property of this source or sink. */private[spark] class MetricsSystem private ( val instance: String, conf: SparkConf, securityMgr: SecurityManager) extends Logging &#123;&#125; MetricsSystem的注释也很好的说明了这些 下图是org.apache.spark.metrics包下的内容 source1234private[spark] trait Source &#123; def sourceName: String // source名称 def metricRegistry: MetricRegistry // metric注册表&#125; MetricRegistry是metrics的内容，暂时只需要知道它维护了一个&lt;String, Metric&gt;注册表1ConcurrentMap&lt;String, Metric&gt; metrics Source功能比较简单，以DAGSchedulerSource为例1234567891011121314151617181920212223242526272829private[scheduler] class DAGSchedulerSource(val dagScheduler: DAGScheduler) extends Source &#123; override val metricRegistry = new MetricRegistry() override val sourceName = "DAGScheduler" metricRegistry.register(MetricRegistry.name("stage", "failedStages"), new Gauge[Int] &#123; override def getValue: Int = dagScheduler.failedStages.size &#125;) metricRegistry.register(MetricRegistry.name("stage", "runningStages"), new Gauge[Int] &#123; override def getValue: Int = dagScheduler.runningStages.size &#125;) metricRegistry.register(MetricRegistry.name("stage", "waitingStages"), new Gauge[Int] &#123; override def getValue: Int = dagScheduler.waitingStages.size &#125;) metricRegistry.register(MetricRegistry.name("job", "allJobs"), new Gauge[Int] &#123; override def getValue: Int = dagScheduler.numTotalJobs &#125;) metricRegistry.register(MetricRegistry.name("job", "activeJobs"), new Gauge[Int] &#123; override def getValue: Int = dagScheduler.activeJobs.size &#125;) /** Timer that tracks the time to process messages in the DAGScheduler's event loop */ val messageProcessingTimer: Timer = metricRegistry.timer(MetricRegistry.name("messageProcessingTime"))&#125; 主要做的，就是定义sourceName，新构造MetricRegistry，然后注册到metrics:ConcurrentMap[String, Metric]注册表 Gauge输出的内容通常是Source构造函数入参的属性，本例中，就是dagScheduler.numTotalJobs、dagScheduler.activeJobs.size sinksource有了数据，还要考虑怎么输出数据，以便后续使用。在spark-shell中的控制台输出，就是一种形式。也可以保留到日志、监控系统等。 12345private[spark] trait Sink &#123; def start(): Unit // 启动sink def stop(): Unit // 停止sink def report(): Unit // 输出到目的地&#125; JmxSink：通过JmxReporter，将度量输出到MBean中.通过Java VisualVM，选择MBeans标签页可以对JmxSink所有注册到JMX中的对象进行管理。 MetricsServlet：在Spark UI的jetty服务中创建ServletContextHandler，将度量数据通过Spark UI展示在浏览器中 Sink的子类，除了MetricsServlet之外，基本都是调用reporter.start,reporter.stop,reporter.report来完成trait里的函数 MetricsSystem//todo 总结可以看到，spark-metric本质上，依赖metrics的Metric，Report进行操作.]]></content>
      <categories>
        <category>spark</category>
        <category>The Art of Spark Kernel Design</category>
      </categories>
      <tags>
        <tag>metric</tag>
        <tag>spark</tag>
        <tag>big data</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark事件总线]]></title>
    <url>%2F2019%2F05%2F21%2Fspark-event-bus%2F</url>
    <content type="text"><![CDATA[ListenerBusspark有一个trait——ListenerBus,事件总线，把事件提交给监听器123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596/** * An event bus which posts events to its listeners. */private[spark] trait ListenerBus[L &lt;: AnyRef, E] extends Logging &#123; private[this] val listenersPlusTimers = new CopyOnWriteArrayList[(L, Option[Timer])] // Marked `private[spark]` for access in tests. private[spark] def listeners = listenersPlusTimers.asScala.map(_._1).asJava /** * Returns a CodaHale metrics Timer for measuring the listener's event processing time. * This method is intended to be overridden by subclasses. */ protected def getTimer(listener: L): Option[Timer] = None /** * Add a listener to listen events. This method is thread-safe and can be called in any thread. */ final def addListener(listener: L): Unit = &#123; listenersPlusTimers.add((listener, getTimer(listener))) &#125; /** * Remove a listener and it won't receive any events. This method is thread-safe and can be called * in any thread. */ final def removeListener(listener: L): Unit = &#123; listenersPlusTimers.asScala.find(_._1 eq listener).foreach &#123; listenerAndTimer =&gt; listenersPlusTimers.remove(listenerAndTimer) &#125; &#125; /** * This can be overriden by subclasses if there is any extra cleanup to do when removing a * listener. In particular AsyncEventQueues can clean up queues in the LiveListenerBus. */ def removeListenerOnError(listener: L): Unit = &#123; removeListener(listener) &#125; /** * Post the event to all registered listeners. The `postToAll` caller should guarantee calling * `postToAll` in the same thread for all events. */ def postToAll(event: E): Unit = &#123; // JavaConverters can create a JIterableWrapper if we use asScala. // However, this method will be called frequently. To avoid the wrapper cost, here we use // Java Iterator directly. val iter = listenersPlusTimers.iterator while (iter.hasNext) &#123; val listenerAndMaybeTimer = iter.next() val listener = listenerAndMaybeTimer._1 val maybeTimer = listenerAndMaybeTimer._2 val maybeTimerContext = if (maybeTimer.isDefined) &#123; maybeTimer.get.time() &#125; else &#123; null &#125; try &#123; doPostEvent(listener, event) if (Thread.interrupted()) &#123; // We want to throw the InterruptedException right away so we can associate the interrupt // with this listener, as opposed to waiting for a queue.take() etc. to detect it. //一旦监听器中断，立即抛异常。而不是等待queue.take()等等，来发现中断 throw new InterruptedException() &#125; &#125; catch &#123; case ie: InterruptedException =&gt; logError(s"Interrupted while posting to $&#123;Utils.getFormattedClassName(listener)&#125;. " + s"Removing that listener.", ie) removeListenerOnError(listener)//删除监听器，线程安全 case NonFatal(e) =&gt; logError(s"Listener $&#123;Utils.getFormattedClassName(listener)&#125; threw an exception", e) &#125; finally &#123; if (maybeTimerContext != null) &#123; maybeTimerContext.stop() &#125; &#125; &#125; &#125; /** * Post an event to the specified listener. `onPostEvent` is guaranteed to be called in the same * thread for all listeners. * 具体对待提交的事件。留给子类继承 */ protected def doPostEvent(listener: L, event: E): Unit private[spark] def findListenersByClass[T &lt;: L : ClassTag](): Seq[T] = &#123; val c = implicitly[ClassTag[T]].runtimeClass listeners.asScala.filter(_.getClass == c).map(_.asInstanceOf[T]).toSeq &#125;&#125; 主要依靠CopyOnWriteArrayList保障线程安全，主动发现InterruptedException，而不是被动等待。具体的操作，留给子类实现doPostEvent ListenerBus的继承体系 SparkListenerBus123456789101112131415161718192021private[spark] trait SparkListenerBus extends ListenerBus[SparkListenerInterface, SparkListenerEvent] &#123; protected override def doPostEvent( listener: SparkListenerInterface, event: SparkListenerEvent): Unit = &#123; event match &#123; case stageSubmitted: SparkListenerStageSubmitted =&gt; listener.onStageSubmitted(stageSubmitted) case stageCompleted: SparkListenerStageCompleted =&gt; listener.onStageCompleted(stageCompleted) case jobStart: SparkListenerJobStart =&gt; listener.onJobStart(jobStart) ... case speculativeTaskSubmitted: SparkListenerSpeculativeTaskSubmitted =&gt; listener.onSpeculativeTaskSubmitted(speculativeTaskSubmitted) case _ =&gt; listener.onOtherEvent(event) &#125; &#125;&#125; SparkListenerBus根据模式匹配，根据不同的SparkListenerEvent,调用SparkListenerInterface的不同方法.doPostEvents是同步调用的，当调度频繁时，可能导致写入延迟，事件丢失 SparkListenerInterface1234567891011121314private[spark] trait SparkListenerInterface &#123; /** * Called when a stage completes successfully or fails, with information on the completed stage. */ def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit /** * Called when a stage is submitted */ def onStageSubmitted(stageSubmitted: SparkListenerStageSubmitted): Unit ...&#125; 官方建议继承SparkListeneror SparkFirehoseListener，而不是直接实现SparkListenerInterface 1234567891011121314151617181920212223242526272829//空实现abstract class SparkListener extends SparkListenerInterface &#123; override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = &#123; &#125; override def onStageSubmitted(stageSubmitted: SparkListenerStageSubmitted): Unit = &#123; &#125; override def onTaskStart(taskStart: SparkListenerTaskStart): Unit = &#123; &#125; ...&#125;//子类只需要重写onEvent一个方法即可public class SparkFirehoseListener implements SparkListenerInterface &#123; public void onEvent(SparkListenerEvent event) &#123; &#125; @Override public final void onStageCompleted(SparkListenerStageCompleted stageCompleted) &#123; onEvent(stageCompleted); &#125; @Override public final void onStageSubmitted(SparkListenerStageSubmitted stageSubmitted) &#123; onEvent(stageSubmitted); &#125; ...&#125; SparkListenerEvent12345678910111213141516171819@DeveloperApi@JsonTypeInfo(use = JsonTypeInfo.Id.CLASS, include = JsonTypeInfo.As.PROPERTY, property = "Event")trait SparkListenerEvent &#123; /* Whether output this event to the event log */ protected[spark] def logEvent: Boolean = true&#125;@DeveloperApicase class SparkListenerStageSubmitted(stageInfo: StageInfo, properties: Properties = null) extends SparkListenerEvent@DeveloperApicase class SparkListenerStageCompleted(stageInfo: StageInfo) extends SparkListenerEvent@DeveloperApicase class SparkListenerTaskStart(stageId: Int, stageAttemptId: Int, taskInfo: TaskInfo) extends SparkListenerEvent...&#125; AsyncEventQueue123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178/** * An asynchronous queue for events. All events posted to this queue will be delivered to the child * listeners in a separate thread. * 所有提交到队列里的事件将在单独的线程里运行 * Delivery will only begin when the `start()` method is called. The `stop()` method should be * called when no more events need to be delivered. */private class AsyncEventQueue( val name: String, conf: SparkConf, metrics: LiveListenerBusMetrics, bus: LiveListenerBus) extends SparkListenerBus with Logging &#123; import AsyncEventQueue._ // Cap the capacity of the queue so we get an explicit error (rather than an OOM exception) if // it's perpetually being added to more quickly than it's being drained. //限定大小，避免OOM。事件先提交到这个队列 private val eventQueue = new LinkedBlockingQueue[SparkListenerEvent]( conf.get(LISTENER_BUS_EVENT_QUEUE_CAPACITY)) // Keep the event count separately, so that waitUntilEmpty() can be implemented properly; // this allows that method to return only when the events in the queue have been fully // processed (instead of just dequeued). private val eventCount = new AtomicLong() /** A counter for dropped events. It will be reset every time we log it. */ private val droppedEventsCounter = new AtomicLong(0L) /** When `droppedEventsCounter` was logged last time in milliseconds. */ @volatile private var lastReportTimestamp = 0L private val logDroppedEvent = new AtomicBoolean(false) private var sc: SparkContext = null private val started = new AtomicBoolean(false) private val stopped = new AtomicBoolean(false) private val droppedEvents = metrics.metricRegistry.counter(s"queue.$name.numDroppedEvents") private val processingTime = metrics.metricRegistry.timer(s"queue.$name.listenerProcessingTime") // Remove the queue size gauge first, in case it was created by a previous incarnation of // this queue that was removed from the listener bus. metrics.metricRegistry.remove(s"queue.$name.size") metrics.metricRegistry.register(s"queue.$name.size", new Gauge[Int] &#123; override def getValue: Int = eventQueue.size() &#125;) private val dispatchThread = new Thread(s"spark-listener-group-$name") &#123;//dispatch在单独线程里运行 setDaemon(true) override def run(): Unit = Utils.tryOrStopSparkContext(sc) &#123;//出了异常，将在新线程里关闭SparkContext dispatch() &#125; &#125; private def dispatch(): Unit = LiveListenerBus.withinListenerThread.withValue(true) &#123; var next: SparkListenerEvent = eventQueue.take()//阻塞，直到取出元素 while (next != POISON_PILL) &#123;//毒丸，优雅的关闭dispatchThread，类似于akka关闭actor val ctx = processingTime.time() try &#123; super.postToAll(next)//提交任务 &#125; finally &#123; ctx.stop() &#125; eventCount.decrementAndGet() next = eventQueue.take() &#125; eventCount.decrementAndGet() &#125; override protected def getTimer(listener: SparkListenerInterface): Option[Timer] = &#123; metrics.getTimerForListenerClass(listener.getClass.asSubclass(classOf[SparkListenerInterface])) &#125; /** * Start an asynchronous thread to dispatch events to the underlying listeners. * * @param sc Used to stop the SparkContext in case the async dispatcher fails. */ private[scheduler] def start(sc: SparkContext): Unit = &#123; if (started.compareAndSet(false, true)) &#123; this.sc = sc dispatchThread.start()//线程开始从队列取出事件并提交 &#125; else &#123; throw new IllegalStateException(s"$name already started!") &#125; &#125; /** * Stop the listener bus. It will wait until the queued events have been processed, but new * events will be dropped. * 等待已入队的事件被处理，新事件将被丢弃 */ private[scheduler] def stop(): Unit = &#123; if (!started.get()) &#123; throw new IllegalStateException(s"Attempted to stop $name that has not yet started!") &#125; if (stopped.compareAndSet(false, true)) &#123; eventCount.incrementAndGet() eventQueue.put(POISON_PILL) &#125; // this thread might be trying to stop itself as part of error handling -- we can't join // in that case. if (Thread.currentThread() != dispatchThread) &#123; dispatchThread.join() &#125; &#125; def post(event: SparkListenerEvent): Unit = &#123;//public方法 if (stopped.get()) &#123; return &#125; eventCount.incrementAndGet() if (eventQueue.offer(event)) &#123;//提交到异步队列，立即返回，不阻塞 return &#125; eventCount.decrementAndGet() droppedEvents.inc() droppedEventsCounter.incrementAndGet() if (logDroppedEvent.compareAndSet(false, true)) &#123; // Only log the following message once to avoid duplicated annoying logs. logError(s"Dropping event from queue $name. " + "This likely means one of the listeners is too slow and cannot keep up with " + "the rate at which tasks are being started by the scheduler.") &#125; logTrace(s"Dropping event $event") val droppedCount = droppedEventsCounter.get if (droppedCount &gt; 0) &#123; // Don't log too frequently if (System.currentTimeMillis() - lastReportTimestamp &gt;= 60 * 1000) &#123;//打印抛弃事件的数量 // There may be multiple threads trying to decrease droppedEventsCounter. // Use "compareAndSet" to make sure only one thread can win. // And if another thread is increasing droppedEventsCounter, "compareAndSet" will fail and // then that thread will update it. if (droppedEventsCounter.compareAndSet(droppedCount, 0)) &#123; val prevLastReportTimestamp = lastReportTimestamp lastReportTimestamp = System.currentTimeMillis() val previous = new java.util.Date(prevLastReportTimestamp) logWarning(s"Dropped $droppedCount events from $name since $previous.") &#125; &#125; &#125; &#125; /** * For testing only. Wait until there are no more events in the queue. * * @return true if the queue is empty. */ def waitUntilEmpty(deadline: Long): Boolean = &#123; while (eventCount.get() != 0) &#123; if (System.currentTimeMillis &gt; deadline) &#123; return false &#125; Thread.sleep(10) &#125; true &#125; override def removeListenerOnError(listener: SparkListenerInterface): Unit = &#123; // the listener failed in an unrecoverably way, we want to remove it from the entire // LiveListenerBus (potentially stopping a queue if it is empty) bus.removeListener(listener) &#125;&#125;private object AsyncEventQueue &#123; val POISON_PILL = new SparkListenerEvent() &#123; &#125;//毒丸，优雅地关闭线程&#125; AsyncEventQueue中的dispatch方法，会调用LiveListenerBus#LiveListenerBus.withinListenerThread.withValue方法，removeListenerOnError实际调用LiveListenerBus#removeListener方法。stop方法不会中断线程，会等待线程执行完，但不接受新事件 LiveListenerBus123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134/** * SparkListenerEvents * Asynchronously passes SparkListenerEvents to registered SparkListeners. * * Until `start()` is called, all posted events are only buffered. Only after this listener bus * has started will events be actually propagated to all attached listeners. This listener bus * is stopped when `stop()` is called, and it will drop further events after stopping. */private[spark] class LiveListenerBus(conf: SparkConf) &#123;... /** * 将监听器加到指定的queue(通过队列的名字) * Add a listener to a specific queue, creating a new queue if needed. Queues are independent * of each other (each one uses a separate thread for delivering events), allowing slower * listeners to be somewhat isolated from others. */ private[spark] def addToQueue( listener: SparkListenerInterface, queue: String): Unit = synchronized &#123; if (stopped.get()) &#123; throw new IllegalStateException("LiveListenerBus is stopped.") &#125; queues.asScala.find(_.name == queue) match &#123; case Some(queue) =&gt; queue.addListener(listener) case None =&gt; val newQueue = new AsyncEventQueue(queue, conf, metrics, this) newQueue.addListener(listener) if (started.get()) &#123; newQueue.start(sparkContext) &#125; queues.add(newQueue) &#125; &#125; def removeListener(listener: SparkListenerInterface): Unit = synchronized &#123; // Remove listener from all queues it was added to, and stop queues that have become empty. queues.asScala .filter &#123; queue =&gt; queue.removeListener(listener) queue.listeners.isEmpty() &#125; .foreach &#123; toRemove =&gt; if (started.get() &amp;&amp; !stopped.get()) &#123; toRemove.stop() &#125; queues.remove(toRemove)//监听器空了的queue要stop，并移除 &#125; &#125; /** Post an event to all queues. */ def post(event: SparkListenerEvent): Unit = &#123; if (stopped.get()) &#123; return &#125; metrics.numEventsPosted.inc() // If the event buffer is null, it means the bus has been started and we can avoid // synchronization and post events directly to the queues. This should be the most // common case during the life of the bus. if (queuedEvents == null) &#123; postToQueues(event) return &#125; // Otherwise, need to synchronize to check whether the bus is started, to make sure the thread // calling start() picks up the new event. synchronized &#123; if (!started.get()) &#123; queuedEvents += event return &#125; &#125; // If the bus was already started when the check above was made, just post directly to the // queues. postToQueues(event) &#125; private def postToQueues(event: SparkListenerEvent): Unit = &#123; val it = queues.iterator() while (it.hasNext()) &#123; it.next().post(event) &#125; &#125;&#125;private[spark] object LiveListenerBus &#123; // Allows for Context to check whether stop() call is made within listener thread val withinListenerThread: DynamicVariable[Boolean] = new DynamicVariable[Boolean](false) private[scheduler] val SHARED_QUEUE = "shared" private[scheduler] val APP_STATUS_QUEUE = "appStatus" private[scheduler] val EXECUTOR_MANAGEMENT_QUEUE = "executorManagement" private[scheduler] val EVENT_LOG_QUEUE = "eventLog"&#125;class DynamicVariable[T](init: T) &#123; private val tl = new InheritableThreadLocal[T] &#123; override def initialValue = init.asInstanceOf[T with AnyRef] &#125; /** Retrieve the current value */ def value: T = tl.get.asInstanceOf[T] /** Set the value of the variable while executing the specified * thunk. * * @param newval The value to which to set the variable * @param thunk The code to evaluate under the new setting */ def withValue[S](newval: T)(thunk: =&gt; S): S = &#123; val oldval = value tl set newval try thunk finally tl set oldval &#125; /** Change the currently bound value, discarding the old value. * Usually withValue() gives better semantics. */ def value_=(newval: T) = tl set newval override def toString: String = "DynamicVariable(" + value + ")"&#125; withValue里对threadlocal的操作，和 RDD.withScope有异曲同工之妙 DAGScheduler、SparkContext等都是LiveListenerBus的事件来源，它们都是通过调用LiveListenerBus的post方法将消息交给异步线程listenerThread处理]]></content>
      <categories>
        <category>spark</category>
        <category>The Art of Spark Kernel Design</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>big data</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark-TransportChannelHandler]]></title>
    <url>%2F2019%2F05%2F21%2Fspark-TransportChannelHandler%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[spark-RpcHandler]]></title>
    <url>%2F2019%2F05%2F21%2Fspark-RpcHandler%2F</url>
    <content type="text"></content>
      <categories>
        <category>spark</category>
        <category>The Art of Spark Kernel Design</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[spark shell]]></title>
    <url>%2F2019%2F05%2F21%2Fspark-shell%2F</url>
    <content type="text"><![CDATA[安装spark的流程就不说了。本篇讲述spark-shell log level在$SPARK_HOME/conf目录中，有log4j.properties文件(如果没有 执行cp log4j.properties.template log4j.properties). spark-shell默认日志级别为WARN,修改log4j.properties如下，以打印更详细的信息1234# Set the default spark-shell log level to WARN. When running the spark-shell, the# log level for this class is used to overwrite the root logger&apos;s log level, so that# the user can have different defaults for the shell and regular Spark apps.log4j.logger.org.apache.spark.repl.Main=INFO run workCount on spark-shell执行$SPARK_HOME/bin/spark-shell，结果如下12345678910111213141516171819202122232425262728293031323334353637383940414243444546(base) joker:spark xmly$ bin/spark-shell19/05/18 15:39:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable19/05/18 15:39:47 INFO SignalUtils: Registered signal handler for INT19/05/18 15:39:53 INFO SparkContext: Running Spark version 2.3.119/05/18 15:39:53 INFO SparkContext: Submitted application: Spark shell19/05/18 15:39:53 INFO SecurityManager: Changing view acls to: xmly19/05/18 15:39:53 INFO SecurityManager: Changing modify acls to: xmly19/05/18 15:39:53 INFO SecurityManager: Changing view acls groups to:19/05/18 15:39:53 INFO SecurityManager: Changing modify acls groups to:19/05/18 15:39:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(xmly); groups with view permissions: Set(); users with modify permissions: Set(xmly); groups with modify permissions: Set()19/05/18 15:39:54 INFO Utils: Successfully started service 'sparkDriver' on port 57338.19/05/18 15:39:54 INFO SparkEnv: Registering MapOutputTracker19/05/18 15:39:54 INFO SparkEnv: Registering BlockManagerMaster19/05/18 15:39:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information19/05/18 15:39:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up19/05/18 15:39:54 INFO DiskBlockManager: Created local directory at /private/var/folders/sb/7sj8qth16j71x82r2m6ctk8r0000gn/T/blockmgr-fcefbfe5-40a6-4058-9724-c3ee2c70119019/05/18 15:39:54 INFO MemoryStore: MemoryStore started with capacity 366.3 MB19/05/18 15:39:54 INFO SparkEnv: Registering OutputCommitCoordinator19/05/18 15:39:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.19/05/18 15:39:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.38.95:404019/05/18 15:39:55 INFO Executor: Starting executor ID driver on host localhost19/05/18 15:39:55 INFO Executor: Using REPL class URI: spark://192.168.38.95:57338/classes19/05/18 15:39:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57339.19/05/18 15:39:55 INFO NettyBlockTransferService: Server created on 192.168.38.95:5733919/05/18 15:39:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy19/05/18 15:39:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.38.95, 57339, None)19/05/18 15:39:55 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.38.95:57339 with 366.3 MB RAM, BlockManagerId(driver, 192.168.38.95, 57339, None)19/05/18 15:39:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.38.95, 57339, None)19/05/18 15:39:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.38.95, 57339, None)19/05/18 15:39:56 INFO EventLoggingListener: Logging events to hdfs://localhost:9000/spark_log/local-1558165194992.lz419/05/18 15:39:56 INFO Main: Created Spark session with Hive supportSpark context Web UI available at http://192.168.38.95:4040Spark context available as 'sc' (master = local[*], app id = local-1558165194992).Spark session available as 'spark'.Welcome to ____ __ / __/__ ___ _____/ /__ _\ \/ _ \/ _ `/ __/ '_/ /___/ .__/\_,_/_/ /_/\_\ version 2.3.1 /_/Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_171)Type in expressions to have them evaluated.Type :help for more information.scala&gt; 从启动日志，可看到SparkContext、SecurityManager、SparkEnv、BlockManagerMasterEndpoint、DiskBlockManager、MemoryStore、SparkUI、Executor、NettyBlockTransferService、BlockManager、等 以下执行workCount123456789101112131415161718192021scala&gt; :paste// Entering paste mode (ctrl-D to finish)val lines = sc.textFile("file:///opt/spark/README.md")val words = lines.flatMap(line =&gt; line.split(" "))val ones = words.map(word =&gt; (word, 1))val counts = ones.reduceByKey(_ + _)// Exiting paste mode, now interpreting.19/05/18 16:09:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 241.5 KB, free 366.1 MB)19/05/18 16:09:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.3 KB, free 366.0 MB)19/05/18 16:09:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.38.95:57718 (size: 23.3 KB, free: 366.3 MB)19/05/18 16:09:07 INFO SparkContext: Created broadcast 0 from textFile at &lt;console&gt;:2419/05/18 16:09:07 INFO FileInputFormat: Total input paths to process : 1lines: org.apache.spark.rdd.RDD[String] = file:///opt/spark/README.md MapPartitionsRDD[1] at textFile at &lt;console&gt;:24words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at &lt;console&gt;:25ones: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at &lt;console&gt;:26counts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at &lt;console&gt;:27scala&gt; 可以看到reducyByKey算子将MapPartitionsRDD转变成ShuffledRDD 通过toDebugString可以看到更详细的RDD转换过程1234567scala&gt; counts.toDebugStringres3: String =(2) ShuffledRDD[4] at reduceByKey at &lt;console&gt;:27 [] +-(2) MapPartitionsRDD[3] at map at &lt;console&gt;:26 [] | MapPartitionsRDD[2] at flatMap at &lt;console&gt;:25 [] | file:///opt/spark/README.md MapPartitionsRDD[1] at textFile at &lt;console&gt;:24 [] | file:///opt/spark/README.md HadoopRDD[0] at textFile at &lt;console&gt;:24 [] 执行action操作1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253scala&gt; counts.collect()19/05/18 16:13:26 INFO SparkContext: Starting job: collect at &lt;console&gt;:2619/05/18 16:13:26 INFO DAGScheduler: Registering RDD 3 (map at &lt;console&gt;:26)19/05/18 16:13:26 INFO DAGScheduler: Got job 0 (collect at &lt;console&gt;:26) with 2 output partitions19/05/18 16:13:26 INFO DAGScheduler: Final stage: ResultStage 1 (collect at &lt;console&gt;:26)19/05/18 16:13:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)19/05/18 16:13:26 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)19/05/18 16:13:26 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at &lt;console&gt;:26), which has no missing parents19/05/18 16:13:26 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.8 KB, free 366.0 MB)19/05/18 16:13:26 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.8 KB, free 366.0 MB)19/05/18 16:13:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.38.95:57718 (size: 2.8 KB, free: 366.3 MB)19/05/18 16:13:26 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:103919/05/18 16:13:26 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at &lt;console&gt;:26) (first 15 tasks are for partitions Vector(0, 1))19/05/18 16:13:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks19/05/18 16:13:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7861 bytes)19/05/18 16:13:26 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7861 bytes)19/05/18 16:13:26 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)19/05/18 16:13:26 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)19/05/18 16:13:26 INFO HadoopRDD: Input split: file:/opt/spark/README.md:0+190419/05/18 16:13:26 INFO HadoopRDD: Input split: file:/opt/spark/README.md:1904+190519/05/18 16:13:26 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1155 bytes result sent to driver19/05/18 16:13:26 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1155 bytes result sent to driver19/05/18 16:13:26 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 202 ms on localhost (executor driver) (1/2)19/05/18 16:13:26 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 223 ms on localhost (executor driver) (2/2)19/05/18 16:13:26 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool19/05/18 16:13:26 INFO DAGScheduler: ShuffleMapStage 0 (map at &lt;console&gt;:26) finished in 0.318 s19/05/18 16:13:26 INFO DAGScheduler: looking for newly runnable stages19/05/18 16:13:26 INFO DAGScheduler: running: Set()19/05/18 16:13:26 INFO DAGScheduler: waiting: Set(ResultStage 1)19/05/18 16:13:26 INFO DAGScheduler: failed: Set()19/05/18 16:13:26 INFO DAGScheduler: Submitting ResultStage 1 (ShuffledRDD[4] at reduceByKey at &lt;console&gt;:27), which has no missing parents19/05/18 16:13:26 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 366.0 MB)19/05/18 16:13:26 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1978.0 B, free 366.0 MB)19/05/18 16:13:26 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.38.95:57718 (size: 1978.0 B, free: 366.3 MB)19/05/18 16:13:26 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:103919/05/18 16:13:27 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (ShuffledRDD[4] at reduceByKey at &lt;console&gt;:27) (first 15 tasks are for partitions Vector(0, 1))19/05/18 16:13:27 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks19/05/18 16:13:27 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7649 bytes)19/05/18 16:13:27 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 7649 bytes)19/05/18 16:13:27 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)19/05/18 16:13:27 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)19/05/18 16:13:27 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks19/05/18 16:13:27 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks19/05/18 16:13:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms19/05/18 16:13:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms19/05/18 16:13:27 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 4673 bytes result sent to driver19/05/18 16:13:27 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 111 ms on localhost (executor driver) (1/2)19/05/18 16:13:27 INFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 4539 bytes result sent to driver19/05/18 16:13:27 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 117 ms on localhost (executor driver) (2/2)19/05/18 16:13:27 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool19/05/18 16:13:27 INFO DAGScheduler: ResultStage 1 (collect at &lt;console&gt;:26) finished in 0.150 s19/05/18 16:13:27 INFO DAGScheduler: Job 0 finished: collect at &lt;console&gt;:26, took 0.753798 sres1: Array[(String, Int)] = Array((package,1), (this,1)...) SparkContext开启job，id是0。 DAGScheduler划分、提交两个stage。第一个stage为ShuffleMapStage，id为0；第二个stage为ResultStage，id为1 每个stage都有两个task，因为2 output partitions(line 4) TaskSchedulerImpl添加task到task set，Executor执行task analyse spark-shellpart of spark-shell12345678910111213141516function main() &#123; if $cygwin; then # Workaround for issue involving JLine and Cygwin # (see http://sourceforge.net/p/jline/bugs/40/). # If you're using the Mintty terminal emulator in Cygwin, may need to set the # "Backspace sends ^H" setting in "Keys" section of the Mintty options # (see https://github.com/sbt/sbt/issues/562). stty -icanon min 1 -echo &gt; /dev/null 2&gt;&amp;1 export SPARK_SUBMIT_OPTS="$SPARK_SUBMIT_OPTS -Djline.terminal=unix" "$&#123;SPARK_HOME&#125;"/bin/spark-submit --class org.apache.spark.repl.Main --name "Spark shell" "$@" stty icanon echo &gt; /dev/null 2&gt;&amp;1 else export SPARK_SUBMIT_OPTS "$&#123;SPARK_HOME&#125;"/bin/spark-submit --class org.apache.spark.repl.Main --name "Spark shell" "$@" fi&#125; spark-shell执行了spark-submit脚本 spark-submit12345678if [ -z "$&#123;SPARK_HOME&#125;" ]; then source "$(dirname "$0")"/find-spark-homefi# disable randomized hash for string in Python 3.3+export PYTHONHASHSEED=0exec "$&#123;SPARK_HOME&#125;"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@" 执行spark-class part of spark-class1234567891011121314151617181920212223# Find the java binaryif [ -n "$&#123;JAVA_HOME&#125;" ]; then RUNNER="$&#123;JAVA_HOME&#125;/bin/java"else if [ "$(command -v java)" ]; then RUNNER="java" else echo "JAVA_HOME is not set" &gt;&amp;2 exit 1 fifibuild_command() &#123; "$RUNNER" -Xmx128m -cp "$LAUNCH_CLASSPATH" org.apache.spark.launcher.Main "$@" printf "%d\0" $?&#125;# Turn off posix mode since it does not allow process substitutionset +o posixCMD=()while IFS= read -d '' -r ARG; do CMD+=("$ARG")done &lt; &lt;(build_command "$@") spark启动以SparkSubmit为主类的JVM进程 jmx监控在spark-shell中找到如下配置:123456# SPARK-4161: scala does not assume use of the java classpath,# so we need to add the "-Dscala.usejavacp=true" flag manually. We# do this specifically for the Spark shell because the scala REPL# has its own class loader, and any additional classpath specified# through spark.driver.extraClassPath is not automatically propagated.SPARK_SUBMIT_OPTS="$SPARK_SUBMIT_OPTS -Dscala.usejavacp=true" 修改为1SPARK_SUBMIT_OPTS="$SPARK_SUBMIT_OPTS -Dscala.usejavacp=true -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=10207 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false" 启动jvisualvm，打开本地或全程JMX的org.apache.spark.deploy.SparkSubmit进程,点击”线程”item，点击”main”线程，然后点击”线程Dump“，会dump线程，拖到最下面1234567891011121314151617181920212223242526272829303132333435363738394041&quot;main&quot; #1 prio=5 os_prio=31 tid=0x00007f9202000800 nid=0x2503 runnable [0x0000700002997000] java.lang.Thread.State: RUNNABLE at java.io.FileInputStream.read0(Native Method) at java.io.FileInputStream.read(FileInputStream.java:207) at jline.internal.NonBlockingInputStream.read(Redefined) - locked &lt;0x000000078368fb18&gt; (a jline.internal.NonBlockingInputStream) at jline.internal.NonBlockingInputStream.read(Redefined) at jline.internal.NonBlockingInputStream.read(Redefined) at jline.internal.InputStreamReader.read(InputStreamReader.java:261) - locked &lt;0x000000078368fb18&gt; (a jline.internal.NonBlockingInputStream) at jline.internal.InputStreamReader.read(InputStreamReader.java:198) - locked &lt;0x000000078368fb18&gt; (a jline.internal.NonBlockingInputStream) at jline.console.ConsoleReader.readCharacter(ConsoleReader.java:2145) at jline.console.ConsoleReader.readLine(ConsoleReader.java:2349) at jline.console.ConsoleReader.readLine(ConsoleReader.java:2269) at scala.tools.nsc.interpreter.jline.InteractiveReader.readOneLine(JLineReader.scala:57) at scala.tools.nsc.interpreter.InteractiveReader$$anonfun$readLine$2.apply(InteractiveReader.scala:37) at scala.tools.nsc.interpreter.InteractiveReader$$anonfun$readLine$2.apply(InteractiveReader.scala:37) at scala.tools.nsc.interpreter.InteractiveReader$.restartSysCalls(InteractiveReader.scala:44) at scala.tools.nsc.interpreter.InteractiveReader$class.readLine(InteractiveReader.scala:37) at scala.tools.nsc.interpreter.jline.InteractiveReader.readLine(JLineReader.scala:28) at scala.tools.nsc.interpreter.ILoop.readOneLine(ILoop.scala:404) at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:413) at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply$mcZ$sp(ILoop.scala:923) at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:909) at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:909) at scala.reflect.internal.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:97) at scala.tools.nsc.interpreter.ILoop.process(ILoop.scala:909) at org.apache.spark.repl.Main$.doMain(Main.scala:76) at org.apache.spark.repl.Main$.main(Main.scala:56) at org.apache.spark.repl.Main.main(Main.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Redefined) at org.apache.spark.deploy.JavaMainApplication.start(Redefined) at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(Redefined) at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(Redefined) at org.apache.spark.deploy.SparkSubmit$.submit(Redefined) at org.apache.spark.deploy.SparkSubmit$.main(Redefined) at org.apache.spark.deploy.SparkSubmit.main(Redefined) 可以看出函数调用顺序:SparkSubmit.main —&gt; Main.main —&gt; ILoop.process]]></content>
      <categories>
        <category>spark</category>
        <category>The Art of Spark Kernel Design</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[spark-TransportConf详解]]></title>
    <url>%2F2019%2F05%2F21%2Fspark-TransportConf%2F</url>
    <content type="text"><![CDATA[TransportConf源码的解释为：A central location that tracks all the settings we expose to users.追踪所有暴露给用户的配置。在SparkContext中，用于给RPC框架提供配置信息 TransportConf有两个属性： ConfigProvider conf：配置提供者 String module：模块 1234567891011121314151617181920212223242526272829303132333435/** * Provides a mechanism for constructing a &#123;@link TransportConf&#125; using some sort of configuration. */public abstract class ConfigProvider &#123; /** Obtains the value of the given config, throws NoSuchElementException if it doesn't exist. */ public abstract String get(String name); /** Returns all the config values in the provider. */ public abstract Iterable&lt;Map.Entry&lt;String, String&gt;&gt; getAll(); public String get(String name, String defaultValue) &#123; try &#123; return get(name); &#125; catch (NoSuchElementException e) &#123; return defaultValue; &#125; &#125; public int getInt(String name, int defaultValue) &#123; return Integer.parseInt(get(name, Integer.toString(defaultValue))); &#125; public long getLong(String name, long defaultValue) &#123; return Long.parseLong(get(name, Long.toString(defaultValue))); &#125; public double getDouble(String name, double defaultValue) &#123; return Double.parseDouble(get(name, Double.toString(defaultValue))); &#125; public boolean getBoolean(String name, boolean defaultValue) &#123; return Boolean.parseBoolean(get(name, Boolean.toString(defaultValue))); &#125;&#125; ConfigProvider比较简单.MapConfigProvider是它的实现12345678910111213141516171819202122232425262728293031public class MapConfigProvider extends ConfigProvider &#123; public static final MapConfigProvider EMPTY = new MapConfigProvider(Collections.emptyMap()); private final Map&lt;String, String&gt; config; public MapConfigProvider(Map&lt;String, String&gt; config) &#123; this.config = new HashMap&lt;&gt;(config); &#125; @Override public String get(String name) &#123; String value = config.get(name); if (value == null) &#123; throw new NoSuchElementException(name); &#125; return value; &#125; @Override public String get(String name, String defaultValue) &#123; String value = config.get(name); return value == null ? defaultValue : value; &#125; @Override public Iterable&lt;Map.Entry&lt;String, String&gt;&gt; getAll() &#123; return config.entrySet(); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637public class TransportConf &#123; private final String SPARK_NETWORK_IO_MODE_KEY; ... private final ConfigProvider conf; private final String module; public TransportConf(String module, ConfigProvider conf) &#123; this.module = module; this.conf = conf; SPARK_NETWORK_IO_MODE_KEY = getConfKey("io.mode"); ... &#125; public int getInt(String name, int defaultValue) &#123; return conf.getInt(name, defaultValue); &#125; public String get(String name, String defaultValue) &#123; return conf.get(name, defaultValue); &#125; private String getConfKey(String suffix) &#123; return "spark." + module + "." + suffix; &#125; public String getModuleName() &#123; return module; &#125; /** IO mode: nio or epoll */ public String ioMode() &#123; return conf.get(SPARK_NETWORK_IO_MODE_KEY, "NIO").toUpperCase(Locale.ROOT); &#125; 可以看出，”spark.” + module + “.suffix”就得到了key，用conf.getXxx(key)得到具体value(个人感觉这是鸡肋，没什么太大作用)。 spark通常使用SparkTransportConf创建TransportConf12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * Provides a utility for transforming from a SparkConf inside a Spark JVM (e.g., Executor, * Driver, or a standalone shuffle service) into a TransportConf with details on our environment * like the number of cores that are allocated to this JVM. */object SparkTransportConf &#123; /** * Specifies an upper bound on the number of Netty threads that Spark requires by default. * In practice, only 2-4 cores should be required to transfer roughly 10 Gb/s, and each core * that we use will have an initial overhead of roughly 32 MB of off-heap memory, which comes * at a premium. * * Thus, this value should still retain maximum throughput and reduce wasted off-heap memory * allocation. It can be overridden by setting the number of serverThreads and clientThreads * manually in Spark's configuration. */ private val MAX_DEFAULT_NETTY_THREADS = 8 /** * Utility for creating a [[TransportConf]] from a [[SparkConf]]. * @param _conf the [[SparkConf]] * @param module the module name * @param numUsableCores if nonzero, this will restrict the server and client threads to only * use the given number of cores, rather than all of the machine's cores. * This restriction will only occur if these properties are not already set. */ def fromSparkConf(_conf: SparkConf, module: String, numUsableCores: Int = 0): TransportConf = &#123; val conf = _conf.clone // Specify thread configuration based on our JVM's allocation of cores (rather than necessarily // assuming we have all the machine's cores). // NB: Only set if serverThreads/clientThreads not already set. val numThreads = defaultNumThreads(numUsableCores) conf.setIfMissing(s"spark.$module.io.serverThreads", numThreads.toString) conf.setIfMissing(s"spark.$module.io.clientThreads", numThreads.toString) new TransportConf(module, new ConfigProvider &#123; override def get(name: String): String = conf.get(name) override def get(name: String, defaultValue: String): String = conf.get(name, defaultValue) override def getAll(): java.lang.Iterable[java.util.Map.Entry[String, String]] = &#123; conf.getAll.toMap.asJava.entrySet() &#125; &#125;) &#125; /** * Returns the default number of threads for both the Netty client and server thread pools. * If numUsableCores is 0, we will use Runtime get an approximate number of available cores. */ private def defaultNumThreads(numUsableCores: Int): Int = &#123; val availableCores = if (numUsableCores &gt; 0) numUsableCores else Runtime.getRuntime.availableProcessors() math.min(availableCores, MAX_DEFAULT_NETTY_THREADS) &#125;&#125; 重点有两个： 如果 numUsableCores &lt;= 0，那么线程数是系统可用处理器的数量，但是系统的cores不可能全部用于网络传输使用，所以这里还将分配给网络传输的内核数量最多限制在8个 最终确认线程数的，以SparkConf的配置优先:12conf.setIfMissing(s"spark.$module.io.serverThreads", numThreads.toString)conf.setIfMissing(s"spark.$module.io.clientThreads", numThreads.toString) 构造一个ConfigProvider的匿名内部类，get的实现实际是代理了SparkConf的get方法]]></content>
      <categories>
        <category>spark</category>
        <category>The Art of Spark Kernel Design</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[spark-initializePipeline]]></title>
    <url>%2F2019%2F05%2F21%2Fspark-initializePipeline%2F</url>
    <content type="text"><![CDATA[在TransportClientFactory的createClient,TransportServer中的init方法，都会用到TransportContext#initializePipeline方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253//被TransportClientFactory#createClient调用public TransportChannelHandler initializePipeline(SocketChannel channel) &#123; return initializePipeline(channel, rpcHandler);&#125;/** * 被TransportServer#init 和 上面的 initializePipeline 调用 * Initializes a client or server Netty Channel Pipeline which encodes/decodes messages and * has a &#123;@link org.apache.spark.network.server.TransportChannelHandler&#125; to handle request or * response messages. * * @param channel The channel to initialize. * @param channelRpcHandler The RPC handler to use for the channel. * * @return Returns the created TransportChannelHandler, which includes a TransportClient that can * be used to communicate on this channel. The TransportClient is directly associated with a * ChannelHandler to ensure all users of the same channel get the same TransportClient object. */public TransportChannelHandler initializePipeline( SocketChannel channel, RpcHandler channelRpcHandler) &#123; try &#123; TransportChannelHandler channelHandler = createChannelHandler(channel, channelRpcHandler); channel.pipeline() .addLast("encoder", ENCODER) .addLast(TransportFrameDecoder.HANDLER_NAME, NettyUtils.createFrameDecoder()) .addLast("decoder", DECODER) .addLast("idleStateHandler", new IdleStateHandler(0, 0, conf.connectionTimeoutMs() / 1000)) // NOTE: Chunks are currently guaranteed to be returned in the order of request, but this // would require more logic to guarantee if this were not part of the same event loop. .addLast("handler", channelHandler); return channelHandler; &#125; catch (RuntimeException e) &#123; logger.error("Error while initializing Netty pipeline", e); throw e; &#125;&#125;/** * Creates the server- and client-side handler which is used to handle both RequestMessages and * ResponseMessages. The channel is expected to have been successfully created, though certain * properties (such as the remoteAddress()) may not be available yet. */private TransportChannelHandler createChannelHandler(Channel channel, RpcHandler rpcHandler) &#123; TransportResponseHandler responseHandler = new TransportResponseHandler(channel); //实际创建client，在与RpcHandler无关，在TransportClientFactory中通过clientRef.set(clientHandler.getClient())设置 TransportClient client = new TransportClient(channel, responseHandler); TransportRequestHandler requestHandler = new TransportRequestHandler(channel, client, rpcHandler, conf.maxChunksBeingTransferred()); //TransportChannelHandler在服务端代理TransportRequestHandler处理RequestMessages，在客户端代理TransportResponseHandler处理ResponseMessages return new TransportChannelHandler(client, responseHandler, requestHandler, conf.connectionTimeoutMs(), closeIdleConnections);&#125; compoment implements TransportFrameDecoder ChannelInboundHandler MessageDecoder ChannelInboundHandler IdleStateHandler ChannelInboundHandler TransportChannelHandler ChannelInboundHandler MessageEncoder ChannelOutboundHandler 在netty中，ChannelInboundHandler按注册的先后顺序来、ChannelOutboundHandler按注册的先后逆序来]]></content>
  </entry>
  <entry>
    <title><![CDATA[spark-TransportClientFactory详解]]></title>
    <url>%2F2019%2F05%2F21%2Fspark-TransportClientFactory%2F</url>
    <content type="text"><![CDATA[TransportClientFactory TransportClientFactory是创建TransportClient的工厂类 构造方法 123456789101112131415161718192021public TransportClientFactory( TransportContext context, List&lt;TransportClientBootstrap&gt; clientBootstraps) &#123; this.context = Preconditions.checkNotNull(context); this.conf = context.getConf(); this.clientBootstraps = Lists.newArrayList(Preconditions.checkNotNull(clientBootstraps)); this.connectionPool = new ConcurrentHashMap&lt;&gt;(); this.numConnectionsPerPeer = conf.numConnectionsPerPeer(); this.rand = new Random(); IOMode ioMode = IOMode.valueOf(conf.ioMode()); this.socketChannelClass = NettyUtils.getClientChannelClass(ioMode); this.workerGroup = NettyUtils.createEventLoop( ioMode, conf.clientThreads(), conf.getModuleName() + "-client"); this.pooledAllocator = NettyUtils.createPooledByteBufAllocator( conf.preferDirectBufs(), false /* allowCache */, conf.clientThreads()); this.metrics = new NettyMemoryMetrics( this.pooledAllocator, conf.getModuleName() + "-client", conf);&#125; TransportClientBootstrap：A bootstrap which is executed on a TransportClient before it is returned to the user. connectionPool： ConcurrentHashMap&lt;SocketAddress, ClientPool&gt;,ClientPool[TransportClient[] clients, Object[] locks] rand: createClient(String remoteHost, int remotePort)中，用于从clientPool.clients中获取TransportClient cachedClient，负载均衡 numConnectionsPerPeer：Number of concurrent connections between two nodes for fetching data.用于构造connectionPool.putIfAbsent(unresolvedAddress, new ClientPool(numConnectionsPerPeer))。 ioMode：从conf中获取，有NIO, EPOLL两种。NIO总是可用，EPOLL只可用于Linux workerGroup：根据Netty的规范，客户端只有worker组.有NioEventLoopGroup、EpollEventLoopGroup两种情况 pooledAllocator：汇集ByteBuf但对本地线程缓存禁用的分配器 TransportClientBootstrap 客户端引导程序在TransportClient返回用户前，做一些操作。可以做一些昂贵操作，因为TransportClient会尽可能重用12345678910111213/** * A bootstrap which is executed on a TransportClient before it is returned to the user. * This enables an initial exchange of information (e.g., SASL authentication tokens) on a once-per- * connection basis. * * Since connections (and TransportClients) are reused as much as possible, it is generally * reasonable to perform an expensive bootstrapping operation, as they often share a lifespan with * the JVM itself. */public interface TransportClientBootstrap &#123; /** Performs the bootstrapping operation, throwing an exception on failure. */ void doBootstrap(TransportClient client, Channel channel) throws RuntimeException;&#125; TransportClientFactory#createClient123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081/** * Factory for creating &#123;@link TransportClient&#125;s by using createClient. * * The factory maintains a connection pool to other hosts and should return the same * TransportClient for the same remote host. It also shares a single worker thread pool for * all TransportClients. * * TransportClients will be reused whenever possible. Prior to completing the creation of a new * TransportClient, all given &#123;@link TransportClientBootstrap&#125;s will be run. */public class TransportClientFactory implements Closeable &#123; /** Create a completely new &#123;@link TransportClient&#125; to the remote address. */ private TransportClient createClient(InetSocketAddress address) throws IOException, InterruptedException &#123; logger.debug("Creating new connection to &#123;&#125;", address); //构建根引导程序Bootstrap，并对其配置 Bootstrap bootstrap = new Bootstrap(); bootstrap.group(workerGroup) .channel(socketChannelClass) // Disable Nagle's Algorithm since we don't want packets to wait .option(ChannelOption.TCP_NODELAY, true) .option(ChannelOption.SO_KEEPALIVE, true) .option(ChannelOption.CONNECT_TIMEOUT_MILLIS, conf.connectionTimeoutMs()) .option(ChannelOption.ALLOCATOR, pooledAllocator); if (conf.receiveBuf() &gt; 0) &#123; bootstrap.option(ChannelOption.SO_RCVBUF, conf.receiveBuf()); &#125; if (conf.sendBuf() &gt; 0) &#123; bootstrap.option(ChannelOption.SO_SNDBUF, conf.sendBuf()); &#125; final AtomicReference&lt;TransportClient&gt; clientRef = new AtomicReference&lt;&gt;(); final AtomicReference&lt;Channel&gt; channelRef = new AtomicReference&lt;&gt;(); //为根引导程序设置管道初始化回调函数 bootstrap.handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override public void initChannel(SocketChannel ch) &#123; TransportChannelHandler clientHandler = context.initializePipeline(ch); clientRef.set(clientHandler.getClient()); channelRef.set(ch); &#125; &#125;); // Connect to the remote server long preConnect = System.nanoTime(); ChannelFuture cf = bootstrap.connect(address); if (!cf.await(conf.connectionTimeoutMs())) &#123; throw new IOException( String.format("Connecting to %s timed out (%s ms)", address, conf.connectionTimeoutMs())); &#125; else if (cf.cause() != null) &#123; throw new IOException(String.format("Failed to connect to %s", address), cf.cause()); &#125; TransportClient client = clientRef.get(); Channel channel = channelRef.get(); assert client != null : "Channel future completed successfully with null client"; // Execute any client bootstraps synchronously before marking the Client as successful. long preBootstrap = System.nanoTime(); logger.debug("Connection to &#123;&#125; successful, running bootstraps...", address); try &#123; for (TransportClientBootstrap clientBootstrap : clientBootstraps) &#123; clientBootstrap.doBootstrap(client, channel);//客户单引导程序 &#125; &#125; catch (Exception e) &#123; // catch non-RuntimeExceptions too as bootstrap may be written in Scala long bootstrapTimeMs = (System.nanoTime() - preBootstrap) / 1000000; logger.error("Exception while bootstrapping client after " + bootstrapTimeMs + " ms", e); client.close(); throw Throwables.propagate(e); &#125; long postBootstrap = System.nanoTime(); logger.info("Successfully created connection to &#123;&#125; after &#123;&#125; ms (&#123;&#125; ms spent in bootstraps)", address, (postBootstrap - preConnect) / 1000000, (postBootstrap - preBootstrap) / 1000000); return client; &#125;&#125; return client之前，在for循环里clientBootstrap.doBootstrap(client, channel)。这是个private方法，在其他两处被调用 创建rpc客户端 TransportClient12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788/** * Create a &#123;@link TransportClient&#125; connecting to the given remote host / port. * * We maintains an array of clients (size determined by spark.shuffle.io.numConnectionsPerPeer) * and randomly picks one to use. If no client was previously created in the randomly selected * spot, this function creates a new client and places it there. * * Prior to the creation of a new TransportClient, we will execute all * &#123;@link TransportClientBootstrap&#125;s that are registered with this factory. * * This blocks until a connection is successfully established and fully bootstrapped. * * Concurrency: This method is safe to call from multiple threads. */public TransportClient createClient(String remoteHost, int remotePort) throws IOException, InterruptedException &#123; // Get connection from the connection pool first. // If it is not found or not active, create a new one. // Use unresolved address here to avoid DNS resolution each time we creates a client. final InetSocketAddress unresolvedAddress = InetSocketAddress.createUnresolved(remoteHost, remotePort);//避免dns解析 // Create the ClientPool if we don't have it yet.只创建ClientPool，不创建TransportClient ClientPool clientPool = connectionPool.get(unresolvedAddress); if (clientPool == null) &#123; connectionPool.putIfAbsent(unresolvedAddress, new ClientPool(numConnectionsPerPeer)); clientPool = connectionPool.get(unresolvedAddress);//并发安全 &#125; int clientIndex = rand.nextInt(numConnectionsPerPeer); TransportClient cachedClient = clientPool.clients[clientIndex];//随机选、负载均衡 if (cachedClient != null &amp;&amp; cachedClient.isActive()) &#123;//ClientPool里有TransportClient // Make sure that the channel will not timeout by updating the last use time of the // handler. Then check that the client is still alive, in case it timed out before // this code was able to update things. TransportChannelHandler handler = cachedClient.getChannel().pipeline() .get(TransportChannelHandler.class); synchronized (handler) &#123; handler.getResponseHandler().updateTimeOfLastRequest(); &#125; if (cachedClient.isActive()) &#123; logger.trace("Returning cached connection to &#123;&#125;: &#123;&#125;", cachedClient.getSocketAddress(), cachedClient); return cachedClient; &#125; &#125; // If we reach here, we don't have an existing connection open. Let's create a new one. // Multiple threads might race here to create new connections. Keep only one of them active. final long preResolveHost = System.nanoTime(); final InetSocketAddress resolvedAddress = new InetSocketAddress(remoteHost, remotePort);//会有dns解析 final long hostResolveTimeMs = (System.nanoTime() - preResolveHost) / 1000000; if (hostResolveTimeMs &gt; 2000) &#123; logger.warn("DNS resolution for &#123;&#125; took &#123;&#125; ms", resolvedAddress, hostResolveTimeMs); &#125; else &#123; logger.trace("DNS resolution for &#123;&#125; took &#123;&#125; ms", resolvedAddress, hostResolveTimeMs); &#125; synchronized (clientPool.locks[clientIndex]) &#123;//锁分段 cachedClient = clientPool.clients[clientIndex]; if (cachedClient != null) &#123; if (cachedClient.isActive()) &#123;//双重检验 logger.trace("Returning cached connection to &#123;&#125;: &#123;&#125;", resolvedAddress, cachedClient); return cachedClient; &#125; else &#123; logger.info("Found inactive connection to &#123;&#125;, creating a new one.", resolvedAddress); &#125; &#125; clientPool.clients[clientIndex] = createClient(resolvedAddress); return clientPool.clients[clientIndex];//调用上面的私有方法 &#125;&#125;/** * 直接创建、不从缓存取 * Create a completely new &#123;@link TransportClient&#125; to the given remote host / port. * This connection is not pooled. * * As with &#123;@link #createClient(String, int)&#125;, this method is blocking. */public TransportClient createUnmanagedClient(String remoteHost, int remotePort) throws IOException, InterruptedException &#123; final InetSocketAddress address = new InetSocketAddress(remoteHost, remotePort); return createClient(address);&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576public class InetSocketAddress extends SocketAddress&#123; // Private implementation class pointed to by all public methods. private static class InetSocketAddressHolder &#123; // The hostname of the Socket Address private String hostname; // The IP address of the Socket Address private InetAddress addr; // The port number of the Socket Address private int port; ... private boolean isUnresolved() &#123; return addr == null; &#125; @Override public final boolean equals(Object obj) &#123; if (obj == null || !(obj instanceof InetSocketAddressHolder)) return false; InetSocketAddressHolder that = (InetSocketAddressHolder)obj; boolean sameIP; if (addr != null) sameIP = addr.equals(that.addr); else if (hostname != null) sameIP = (that.addr == null) &amp;&amp; hostname.equalsIgnoreCase(that.hostname); else sameIP = (that.addr == null) &amp;&amp; (that.hostname == null); return sameIP &amp;&amp; (port == that.port); &#125; @Override public final int hashCode() &#123; if (addr != null) return addr.hashCode() + port; if (hostname != null) return hostname.toLowerCase().hashCode() + port; return port; &#125; &#125; private final transient InetSocketAddressHolder holder; public InetSocketAddress(String hostname, int port) &#123; checkHost(hostname); InetAddress addr = null; String host = null; try &#123; addr = InetAddress.getByName(hostname);//DNS解析 &#125; catch(UnknownHostException e) &#123; host = hostname; &#125; holder = new InetSocketAddressHolder(host, addr, checkPort(port)); &#125; // private constructor for creating unresolved instances private InetSocketAddress(int port, String hostname) &#123; holder = new InetSocketAddressHolder(hostname, null, port);//无dns解析 &#125; public static InetSocketAddress createUnresolved(String host, int port) &#123; return new InetSocketAddress(checkPort(port), checkHost(host)); &#125; /** * Returns a hashcode for this socket address. * * @return a hash code value for this socket address. */ @Override public final int hashCode() &#123; return holder.hashCode(); &#125;&#125;]]></content>
      <categories>
        <category>spark</category>
        <category>The Art of Spark Kernel Design</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[spark-TransportServer]]></title>
    <url>%2F2019%2F05%2F21%2Fspark-TransportServer%2F</url>
    <content type="text"><![CDATA[TransportContext有四个重载方法，用来创建TransportServer12345678910111213141516171819/** Create a server which will attempt to bind to a specific port. */public TransportServer createServer(int port, List&lt;TransportServerBootstrap&gt; bootstraps) &#123; return new TransportServer(this, null, port, rpcHandler, bootstraps);&#125;/** Create a server which will attempt to bind to a specific host and port. */public TransportServer createServer( String host, int port, List&lt;TransportServerBootstrap&gt; bootstraps) &#123; return new TransportServer(this, host, port, rpcHandler, bootstraps);&#125;/** Creates a new server, binding to any available ephemeral port. */public TransportServer createServer(List&lt;TransportServerBootstrap&gt; bootstraps) &#123; return createServer(0, bootstraps);&#125;public TransportServer createServer() &#123; return createServer(0, new ArrayList&lt;&gt;());&#125; TransportServer123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118/** * Server for the efficient, low-level streaming service. */public class TransportServer implements Closeable &#123; private static final Logger logger = LoggerFactory.getLogger(TransportServer.class); private final TransportContext context; private final TransportConf conf; private final RpcHandler appRpcHandler; private final List&lt;TransportServerBootstrap&gt; bootstraps; private ServerBootstrap bootstrap; private ChannelFuture channelFuture; private int port = -1; private NettyMemoryMetrics metrics; /** * Creates a TransportServer that binds to the given host and the given port, or to any available * if 0. If you don't want to bind to any special host, set "hostToBind" to null. * */ public TransportServer( TransportContext context, String hostToBind, int portToBind, RpcHandler appRpcHandler, List&lt;TransportServerBootstrap&gt; bootstraps) &#123; this.context = context; this.conf = context.getConf(); this.appRpcHandler = appRpcHandler;//Handler for sendRPC() messages sent by TransportClients. this.bootstraps = Lists.newArrayList(Preconditions.checkNotNull(bootstraps)); try &#123; init(hostToBind, portToBind); &#125; catch (RuntimeException e) &#123; JavaUtils.closeQuietly(this); throw e; &#125; &#125; public int getPort() &#123; if (port == -1) &#123; throw new IllegalStateException("Server not initialized"); &#125; return port; &#125; private void init(String hostToBind, int portToBind) &#123; IOMode ioMode = IOMode.valueOf(conf.ioMode());//nio epoll EventLoopGroup bossGroup = NettyUtils.createEventLoop(ioMode, conf.serverThreads(), conf.getModuleName() + "-server"); EventLoopGroup workerGroup = bossGroup; //✈汇集ByteBuf但对本地线程缓存禁用的分配器 PooledByteBufAllocator allocator = NettyUtils.createPooledByteBufAllocator( conf.preferDirectBufs(), true /* allowCache */, conf.serverThreads()); bootstrap = new ServerBootstrap() .group(bossGroup, workerGroup)//服务端同时需要bossGroup, workerGroup .channel(NettyUtils.getServerChannelClass(ioMode)) .option(ChannelOption.ALLOCATOR, allocator) .childOption(ChannelOption.ALLOCATOR, allocator); this.metrics = new NettyMemoryMetrics( allocator, conf.getModuleName() + "-server", conf);//性能统计，大数据组件常用metrics-core if (conf.backLog() &gt; 0) &#123; bootstrap.option(ChannelOption.SO_BACKLOG, conf.backLog()); &#125; if (conf.receiveBuf() &gt; 0) &#123; bootstrap.childOption(ChannelOption.SO_RCVBUF, conf.receiveBuf()); &#125; if (conf.sendBuf() &gt; 0) &#123; bootstrap.childOption(ChannelOption.SO_SNDBUF, conf.sendBuf()); &#125; bootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123;//管道初始化回调函数 @Override protected void initChannel(SocketChannel ch) &#123; RpcHandler rpcHandler = appRpcHandler; for (TransportServerBootstrap bootstrap : bootstraps) &#123; rpcHandler = bootstrap.doBootstrap(ch, rpcHandler); &#125; context.initializePipeline(ch, rpcHandler);//TransportClientFactory#createClient也会用到 &#125; &#125;); InetSocketAddress address = hostToBind == null ? new InetSocketAddress(portToBind): new InetSocketAddress(hostToBind, portToBind); channelFuture = bootstrap.bind(address); channelFuture.syncUninterruptibly(); port = ((InetSocketAddress) channelFuture.channel().localAddress()).getPort(); logger.debug("Shuffle server started on port: &#123;&#125;", port); &#125; public MetricSet getAllMetrics() &#123; return metrics;//大数据常用统计组件 &#125; @Override public void close() &#123; if (channelFuture != null) &#123; // close is a local operation and should finish within milliseconds; timeout just to be safe channelFuture.channel().close().awaitUninterruptibly(10, TimeUnit.SECONDS); channelFuture = null; &#125; if (bootstrap != null &amp;&amp; bootstrap.group() != null) &#123; bootstrap.group().shutdownGracefully(); &#125; if (bootstrap != null &amp;&amp; bootstrap.childGroup() != null) &#123; bootstrap.childGroup().shutdownGracefully(); &#125; bootstrap = null; &#125;&#125; 可以看到，无论是TransportClient、TransportServer，都大量使用了netty 大数据技术栈spark，hadoop等，都用到metrics-core.后面争取学一学 metrics-core github metrics-core官网]]></content>
      <categories>
        <category>spark</category>
        <category>The Art of Spark Kernel Design</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>big data</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark rpc]]></title>
    <url>%2F2019%2F05%2F20%2Fspark-rpc%2F</url>
    <content type="text"><![CDATA[spark内置RPC框架Spark组件间的消息互通、用户文件与Jar包的上传、节点间的Shuffle过程、Block数据的复制与备份等都用到Spark RPC Spark RPC各组件简介TransportContext：Contains the context to create a TransportServer, TransportClientFactory, and to setup Netty Channel pipelines with a TransportChannelHandler. TransportConf：A central location that tracks all the settings we expose to users。用于创建TransportClientFactory、TransportServer RpcHandler：Handler for sendRPC() messages sent by link org.apache.spark.network.client.TransportClients.只用于创建TransportServer MessageEncoder：服务端用于给server-to-client responses编码，无状态，多线程安全 MessageDecoder：客户端将server-to-client responses解码，无状态，多线程安全 RpcResponseCallback：Callback for the result of a single RPC. This will be invoked once with either success or failure TransportClientBootstrap：A bootstrap which is executed on a TransportClient before it is returned to the user. TransportRequestHandler：A handler that processes requests from clients and writes chunk data back. TransportResponseHandler：Handler that processes server responses TransportChannelHandler：The single Transport-level Channel handler which is used for delegating requests to the TransportRequestHandler and responses to the TransportResponseHandler. TransportServerBootstrap：A bootstrap which is executed on a TransportServer’s client channel once a client connects to the server. TransportClientFactory包含针对每个Socket地址的连接池12345678910111213141516private final ConcurrentHashMap&lt;SocketAddress, ClientPool&gt; connectionPool;/** A simple data structure to track the pool of clients between two peer nodes. */private static class ClientPool &#123; TransportClient[] clients; Object[] locks; //对不同的TransportClient采用不同的锁，类似于锁分段，降低并发下的锁争用 ClientPool(int size) &#123; clients = new TransportClient[size]; locks = new Object[size]; for (int i = 0; i &lt; size; i++) &#123; locks[i] = new Object(); &#125; &#125;&#125; Spark RPC各组件详解spark-TransportConf spark-TransportClientFactory spark-TransportServer spark-管道初始化 先告一段落 RPC传输管道处理器TransportChannelHandler详解服务端RpcHandler详解服务端引导程序TransportServerBootstrap客户端TransportClient详解]]></content>
      <categories>
        <category>spark</category>
        <category>The Art of Spark Kernel Design</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>big data</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark-SparkConf]]></title>
    <url>%2F2019%2F05%2F20%2Fspark-SparkConf%2F</url>
    <content type="text"><![CDATA[spark配置SparkConf是Spark的配置类.1private val settings = new ConcurrentHashMap[String, String]() 配置以[String, String]的形式保存在ConcurrentHashMap中。配置Spark参数有以下三种形式： 1. 系统属性中以&quot;spark.&quot;开头的属性 2. 调用SparkConf的api 3. 从其他SparkConf克隆 其中，直接通过SparkConf设置的属性优先级比系统属性优先级高 系统属性中以”spark.”开头的属性1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465class SparkConf(loadDefaults: Boolean) extends Cloneable with Logging with Serializable &#123; import SparkConf._ /** Create a SparkConf that loads defaults from system properties and the classpath */ def this() = this(true) private val settings = new ConcurrentHashMap[String, String]() @transient private lazy val reader: ConfigReader = &#123; val _reader = new ConfigReader(new SparkConfigProvider(settings)) _reader.bindEnv(new ConfigProvider &#123; override def get(key: String): Option[String] = Option(getenv(key)) &#125;) _reader &#125; if (loadDefaults) &#123; loadFromSystemProperties(false) &#125; private[spark] def loadFromSystemProperties(silent: Boolean): SparkConf = &#123; // Load any spark.* system properties for ((key, value) &lt;- Utils.getSystemProperties if key.startsWith("spark.")) &#123; set(key, value, silent) &#125; this &#125; private[spark] def set(key: String, value: String, silent: Boolean): SparkConf = &#123; if (key == null) &#123; throw new NullPointerException("null key") &#125; if (value == null) &#123; throw new NullPointerException("null value for " + key) &#125; if (!silent) &#123; logDeprecationWarning(key) &#125; settings.put(key, value) this &#125; ...&#125;/** * Various utility methods used by Spark. */private[spark] object Utils extends Logging &#123; ... /** * Returns the system properties map that is thread-safe to iterator over. It gets the * properties which have been set explicitly, as well as those for which only a default value * has been defined. */ def getSystemProperties: Map[String, String] = &#123; System.getProperties.stringPropertyNames().asScala .map(key =&gt; (key, System.getProperty(key))).toMap &#125; ...&#125; 可以看到，loadDefaults参数决定是否使用系统参数中以 spark.开头的属性，并将其保存到settings中。默认是true 直接设置SparkConf123456789101112131415161718192021222324252627282930313233343536373839404142/** Set a configuration variable. */def set(key: String, value: String): SparkConf = &#123; set(key, value, false)&#125;private[spark] def set(key: String, value: String, silent: Boolean): SparkConf = &#123; if (key == null) &#123; throw new NullPointerException("null key") &#125; if (value == null) &#123; throw new NullPointerException("null value for " + key) &#125; if (!silent) &#123; logDeprecationWarning(key) &#125; settings.put(key, value) this&#125;/** * The master URL to connect to, such as "local" to run locally with one thread, "local[4]" to * run locally with 4 cores, or "spark://master:7077" to run on a Spark standalone cluster. */def setMaster(master: String): SparkConf = &#123; set("spark.master", master)&#125;/** Set a name for your application. Shown in the Spark web UI. */def setAppName(name: String): SparkConf = &#123; set("spark.app.name", name)&#125;/** Set JAR files to distribute to the cluster. */def setJars(jars: Seq[String]): SparkConf = &#123; for (jar &lt;- jars if (jar == null)) logWarning("null jar passed to SparkContext constructor") set("spark.jars", jars.filter(_ != null).mkString(","))&#125;/** Set JAR files to distribute to the cluster. (Java-friendly version.) */def setJars(jars: Array[String]): SparkConf = &#123; setJars(jars.toSeq)&#125; 部分setXxx自动设置了key clone当spark多个组件共享配置，可以 把SparkConf变量共享，或通过参数传递给其他组件。但并发下，ConcurrentHashMap性能下降 使用clone(),优雅，避免复制代码臃肿 12345678/** Copy this object */override def clone: SparkConf = &#123; val cloned = new SparkConf(false)//不使用系统配置，下面的foreach会完全复制配置 settings.entrySet().asScala.foreach &#123; e =&gt; cloned.set(e.getKey(), e.getValue(), true) &#125; cloned&#125;]]></content>
      <categories>
        <category>spark</category>
        <category>The Art of Spark Kernel Design</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>big data</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark设计理念和基本架构]]></title>
    <url>%2F2019%2F05%2F19%2Fspark-design-concept-basic-architecture%2F</url>
    <content type="text"><![CDATA[spark核心功能 基础设施 包括SparkConf、内置RPC框架、事件总线(ListenerBus)、度量系统 1. SparkConf管理Spark配置信息 2. RPC框架由netty实现，有同步、异步多种实现,跨机器节点不同组件的通信设施，用于各组件间的通信 3. ListenerBus是sparkContext内部各组件使用事件-监听器模式异步调用 4. 度量系统(MetricSystem)，由多种度量源、度量输出(Source、Sink)构成，完成各组件运行期状态的监控 SparkContext 隐藏了网络通信、分布式部署、消息通信、存储体系、计算引擎、度量系统、文件服务、webui SparkEnv task运行必须的组件。封装了RPC环境(RpcEnv)、序列化管理器、广播管理器(BroadcastManager)、Map输出跟踪器(MapOutputTracker)、存储体系、度量系统(MetricsSystem)、输出提交协调器(OutputCommitCoordinator) 存储体系 优先内存、其次磁盘.Spark的内存存储空间、执行存储空间之间可以是软边界，资源紧张的一方可以借用另一方的空间。还提供Tungsten的实现，直接操作os的内存，空间的分配、释放更加迅速。而且省去在堆内分配java对象，更有效利用系统内存资源 调度系统 内置于SparkContext的DAGScheduler、TaskScheduler构成 DAGScheduler负责创建job，将DAG中的RDD划分到不同Stage、给Stage创建对应的task，批量提交task TaskScheduler按FIFO、FAIR等调度算法对批量Task进行调度，为task分配资源，发送task到Executor 计算引擎 由内存管理器(MemoryManager)、Tungsten、任务内存管理器(TaskMemoryManager)、Task、外部排序器(ExternalSorter)、Shuffle管理器(ShuffleManager)组成 MemoryManager为存储内存、计算引擎中的执行内存呢提供支持、管理 Tungsten 用于存储、计算执行 TaskMemoryManager为分配给单个task的内存资源进行更细粒度的管理控制 ExternalSorter在map、reduce端对ShuffleMapTask计算得到的中间结果排序、聚合 ShuffleManager将各个分区ShuffleMapTask产生的中间结果持久化到磁盘，在reduce端按分区远程拉取中间结果 spark扩展功能spark sql、spark streaming、graphx、mllib等 基本架构 ClusterManager 负责集群资源的管理分配。分配的资源属于一级分配，将worker的内存、cpu等资源分配给application。不负责对executor的资源分配 worker 将内存、cpu通过注册机制告知ClusterManager；创建executor，分配资源、任务给executor；同步资源信息、executor状态给ClusterManager。standalone模式下，master将worker的资源分配给application后，将命令worker启动CoarseGrainedExecutorBackend进程(此进程创建executor实例), executor，执行任务，与worker、driver信息同步 driver，application的驱动程序。application通过driver与ClusterManager、executor通信。可运行在application中，或由app提交给ClusterManager，由ClusterManager安排worker运行 application：转换RDD，构建DAG，通过driver注册到ClusterManager。ClusterManager根据app的资源需求，通过一级分配将executor、内存、cpu分配给app；driver通过二级分配将executor等资源分配给task。app通过driver告诉executor执行任务 具体流程通过SparkContext提交的用户应用程序，首先会通过RpcEnv向ClusterManager注册应用(Application)，并告知需要的资源数量。ClusterManager给application分配executor资源，并在worker上启动CoarseGrainedExecutorBackend进程,进程启动过程中通过RpcEnv向driver注册executor的资源信息。 TaskScheduler保存executor的资源信息。SparkContext构建RDD的lineage和DAG。DAG提交给DAGScheduler，DAGScheduler给提交的DAG创建job，将dag划分为Stage。DAGScheduler根据RDD内partition的数量创建task并批量提交给TaskScheduler。TaskScheduler对task按FIFO、FAIR等调度算法调度，将task发送给executor sparkContext会在RDD转换前用BlockManager和BroadcastManager将任务的hadoop配置进行广播]]></content>
      <categories>
        <category>spark</category>
        <category>The Art of Spark Kernel Design</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>big data</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark withScope and clean]]></title>
    <url>%2F2019%2F05%2F17%2Fspark-withScope-and-clean%2F</url>
    <content type="text"><![CDATA[where are withScope {} and sc.clean(f)在RDD中，充斥着大量的 withScope {},sc.clean(f)。如下所示：123456789101112131415161718192021222324252627/** * Return a new RDD by applying a function to all elements of this RDD. */def map[U: ClassTag](f: T =&gt; U): RDD[U] = withScope &#123; val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (context, pid, iter) =&gt; iter.map(cleanF))&#125;/** * Return a new RDD by first applying a function to all elements of this * RDD, and then flattening the results. */def flatMap[U: ClassTag](f: T =&gt; TraversableOnce[U]): RDD[U] = withScope &#123; val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (context, pid, iter) =&gt; iter.flatMap(cleanF))&#125;/** * Return a new RDD containing only the elements that satisfy a predicate. */def filter(f: T =&gt; Boolean): RDD[T] = withScope &#123; val cleanF = sc.clean(f) new MapPartitionsRDD[T, T]( this, (context, pid, iter) =&gt; iter.filter(cleanF), preservesPartitioning = true)&#125; 大多数情况下，这两个function不影响你阅读源码。但仍有必要弄懂它们是干什么的！ withScope {}org.apache.spark.rdd.RDD123456789101112/** * Execute a block of code in a scope such that all new RDDs created in this body will * be part of the same scope. For more detail, see &#123;&#123;org.apache.spark.rdd.RDDOperationScope&#125;&#125;. * * Note: Return statements are NOT allowed in the given body. * * 在scope中执行代码块，这样所有body中创建的RDDs会成为相同scope的一部分。 * 详见 &#123;&#123;org.apache.spark.rdd.RDDOperationScope&#125;&#125; * * 注意：给定body中不允许有return语句 */private[spark] def withScope[U](body: =&gt; U): U = RDDOperationScope.withScope[U](sc)(body) 源码及解析如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168package org.apache.spark.rddimport java.util.concurrent.atomic.AtomicIntegerimport com.fasterxml.jackson.annotation.&#123;JsonIgnore, JsonInclude, JsonPropertyOrder&#125;import com.fasterxml.jackson.annotation.JsonInclude.Includeimport com.fasterxml.jackson.databind.ObjectMapperimport com.fasterxml.jackson.module.scala.DefaultScalaModuleimport com.google.common.base.Objectsimport org.apache.spark.SparkContextimport org.apache.spark.internal.Logging/** * A general, named code block representing an operation that instantiates RDDs. * * All RDDs instantiated in the corresponding code block will store a pointer to this object. * Examples include, but will not be limited to, existing RDD operations, such as textFile, * reduceByKey, and treeAggregate. * * An operation scope may be nested in other scopes. For instance, a SQL query may enclose * scopes associated with the public RDD APIs it uses under the hood. * * There is no particular relationship between an operation scope and a stage or a job. * A scope may live inside one stage (e.g. map) or span across multiple jobs (e.g. take). * * 一个通用的、被命名的代码块表示一个实例化RDDs的操作(就是spark算子)。 * * 在相应代码块中示例化的所有RDDs将存储指向该对象的指针(就是指向spark算子)。 * 包括但不限于已存在的RDD operation,如textFile,reduceByKey, and treeAggregate. * * 一个操作的scope可能嵌入其他scopes。例如，一个SQL查询可能包含它在hood(引擎盖，有sql执行引擎之说)下使用的public RDD APIs关联的scope。 * * 操作的scope与stage、job之间没有特别的关联。scope可能只存活于一个stage(例如map)，或跨多个jobs(如take) */@JsonInclude(Include.NON_NULL)@JsonPropertyOrder(Array("id", "name", "parent"))private[spark] class RDDOperationScope( val name: String, val parent: Option[RDDOperationScope] = None, val id: String = RDDOperationScope.nextScopeId().toString) &#123;//id自增 def toJson: String = &#123; RDDOperationScope.jsonMapper.writeValueAsString(this) &#125; /** * Return a list of scopes that this scope is a part of, including this scope itself. * The result is ordered from the outermost scope (eldest ancestor) to this scope. * * 返回scopes的list，包含本scope。结果按照从最外层的scope(最老的祖先)到本scope的顺序排序 */ @JsonIgnore def getAllScopes: Seq[RDDOperationScope] = &#123; parent.map(_.getAllScopes).getOrElse(Seq.empty) ++ Seq(this) &#125; override def equals(other: Any): Boolean = &#123; other match &#123; case s: RDDOperationScope =&gt; id == s.id &amp;&amp; name == s.name &amp;&amp; parent == s.parent case _ =&gt; false &#125; &#125; override def hashCode(): Int = Objects.hashCode(id, name, parent) override def toString: String = toJson&#125;/** * A collection of utility methods to construct a hierarchical representation of RDD scopes. * An RDD scope tracks the series of operations that created a given RDD. * * 工具方法的集合，来构造RDD scopes的分层表示. * 一个RDD scope追踪创建给定RDD的一系列算子. */private[spark] object RDDOperationScope extends Logging &#123; private val jsonMapper = new ObjectMapper().registerModule(DefaultScalaModule) private val scopeCounter = new AtomicInteger(0) def fromJson(s: String): RDDOperationScope = &#123; jsonMapper.readValue(s, classOf[RDDOperationScope]) &#125; /** Return a globally unique operation scope ID. 返回全局唯一的operation scope ID*/ def nextScopeId(): Int = scopeCounter.getAndIncrement /** * Execute the given body such that all RDDs created in this body will have the same scope. * The name of the scope will be the first method name in the stack trace that is not the * same as this method's. * * Note: Return statements are NOT allowed in body. * * 执行给定的body，body中创建的所有RDDs将会拥有相同的scope。scope的名字是调用链里第一个不是"withScope"的方法的名字。 * * 注意:body不允许return声明 */ private[spark] def withScope[T]( sc: SparkContext, allowNesting: Boolean = false)(body: =&gt; T): T = &#123; val ourMethodName = "withScope" val callerMethodName = Thread.currentThread.getStackTrace()//调用链 倒序 .dropWhile(_.getMethodName != ourMethodName) .find(_.getMethodName != ourMethodName) .map(_.getMethodName) .getOrElse &#123; // Log a warning just in case, but this should almost certainly never happen logWarning("No valid method name for this RDD operation scope!") "N/A" &#125; withScope[T](sc, callerMethodName, allowNesting, ignoreParent = false)(body) &#125; /** * Execute the given body such that all RDDs created in this body will have the same scope. * * 执行给定的body，body里创建的RDDs都拥有相同的scope * * If nesting is allowed, any subsequent calls to this method in the given body will instantiate * child scopes that are nested within our scope. Otherwise, these calls will take no effect. * * 如果允许嵌套，body中方法的后续调用(withScope)将实例化子scopes，子scopes都嵌套在我们的scope中。否则，调用没有影响 * * Additionally, the caller of this method may optionally ignore the configurations and scopes * set by the higher level caller. In this case, this method will ignore the parent caller's * intention to disallow nesting, and the new scope instantiated will not have a parent. This * is useful for scoping physical operations in Spark SQL, for instance. * * 另外，方法的调用者可选择忽略被 更高层调用者 设定的配置和scopes。这种情况下，方法将忽略父调用者禁止嵌套的意图，新的实例化 * 的scope将不再有parent。这对在spark sql中的物理operations很有作用 * * Note: Return statements are NOT allowed in body. * * 此方法会嵌套执行，body里面会套body */ private[spark] def withScope[T]( sc: SparkContext, name: String, allowNesting: Boolean, ignoreParent: Boolean)(body: =&gt; T): T = &#123; // Save the old scope to restore it later 保存老的scope，便于后面恢复 val scopeKey = SparkContext.RDD_SCOPE_KEY val noOverrideKey = SparkContext.RDD_SCOPE_NO_OVERRIDE_KEY val oldScopeJson = sc.getLocalProperty(scopeKey)//从ThreadLocal中获取 val oldScope = Option(oldScopeJson).map(RDDOperationScope.fromJson)//json反序列化 val oldNoOverride = sc.getLocalProperty(noOverrideKey)//从ThreadLocal中获取 try &#123; if (ignoreParent) &#123;//忽略父scope // Ignore all parent settings and scopes and start afresh with our own root scope sc.setLocalProperty(scopeKey, new RDDOperationScope(name).toJson)//忽略parent settings and scopes，开始新的root scope &#125; else if (sc.getLocalProperty(noOverrideKey) == null) &#123;//allowNesting == true 或 当前thread第一次执行withScope，此时oldScope为 None // Otherwise, set the scope only if the higher level caller allows us to do so sc.setLocalProperty(scopeKey, new RDDOperationScope(name, oldScope).toJson)//保存父scope &#125; // Optionally disallow the child body to override our scope if (!allowNesting) &#123; sc.setLocalProperty(noOverrideKey, "true") &#125; body//执行body 如果allowNesting ignoreParent都为false 则只执行body，scope保持不变 &#125; finally &#123; // Remember to restore any state that was modified before exiting 恢复状态 sc.setLocalProperty(scopeKey, oldScopeJson) sc.setLocalProperty(noOverrideKey, oldNoOverride) &#125; &#125;&#125; 其核心功能，是将RDDOperationScope对象放入ThreadLocal,name是withScope前的函数名. 可用来做DAG可视化 DAG visualization on SparkUI。如的SparkUI sc.clean(f)作用：闭包清理，降低网络io,提高executor的内存效率 在spark分布式环境中，如果引用的外部变量不可序列化，就不能正确发送到worker节点上去。一些没有用到的引用不需要发送到worker上.ClosureCleaner.clean()通过递归遍历闭包里面的引用，检查不能serializable的, 去除unused的引用； 分析： 闭包：函数引用了外部变量。 源码解析 123456789101112131415161718192021/** * Clean a closure to make it ready to be serialized and sent to tasks * (removes unreferenced variables in $outer's, updates REPL variables) * If &lt;tt&gt;checkSerializable&lt;/tt&gt; is set, &lt;tt&gt;clean&lt;/tt&gt; will also proactively * check to see if &lt;tt&gt;f&lt;/tt&gt; is serializable and throw a &lt;tt&gt;SparkException&lt;/tt&gt; * if not. * * 清理一个闭包，让它可以被序列化并发送给tasks（删除未引用的外部变量、更新的REPL变量） * 如果checkSerializable被设定，clean方法将主动主动检查函数f是否可序列化。否则抛出SparkException * * @param f the closure to clean * @param checkSerializable whether or not to immediately check &lt;tt&gt;f&lt;/tt&gt; for serializability * @throws SparkException if &lt;tt&gt;checkSerializable&lt;/tt&gt; is set but &lt;tt&gt;f&lt;/tt&gt; is not * serializable * @return the cleaned closure */ private[spark] def clean[F &lt;: AnyRef](f: F, checkSerializable: Boolean = true): F = &#123; ClosureCleaner.clean(f, checkSerializable) f &#125; org.apache.spark.util.ClosureCleaner1234567private def clean( func: AnyRef, checkSerializable: Boolean, cleanTransitively: Boolean, accessedFields: Map[Class[_], Set[String]]): Unit = &#123; 源码看不懂&#125; What does Closure.cleaner (func) mean in Spark? Spark源码分析之ClosureCleaner(推荐，有反编译) spark: ClosureCleaner.clean()]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>big data</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark transformation and action overview]]></title>
    <url>%2F2019%2F05%2F15%2Fspark-transformation-and-action-overview%2F</url>
    <content type="text"><![CDATA[action 与 transformation learn spark by an example中提到了 transformation 和 action 两种操作，意为准换、行动。org.apache.spark.rdd.RDD 包含这两种算子。可以通过函数返回值来区分这两种算子： transformation：返回RDD，如map，flatMap，union等 action：返回具体的值，如count，collect等 很多资料显示，transformation延迟执行，action立即执行。这是为什么呢？ 以map为例：123456789101112131415161718192021222324252627//org.apache.spark.rdd.RDDdef map[U: ClassTag](f: T =&gt; U): RDD[U] = withScope &#123; val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (context, pid, iter) =&gt; iter.map(cleanF))&#125;/** * An RDD that applies the provided function to every partition of the parent RDD. */private[spark] class MapPartitionsRDD[U: ClassTag, T: ClassTag]( var prev: RDD[T], f: (TaskContext, Int, Iterator[T]) =&gt; Iterator[U], // (TaskContext, partition index, iterator) preservesPartitioning: Boolean = false) extends RDD[U](prev) &#123; override val partitioner = if (preservesPartitioning) firstParent[T].partitioner else None override def getPartitions: Array[Partition] = firstParent[T].partitions override def compute(split: Partition, context: TaskContext): Iterator[U] = f(context, split.index, firstParent[T].iterator(split, context)) override def clearDependencies() &#123; super.clearDependencies() prev = null &#125;&#125; 可见，map算子只是new了一个新 MapPartitionsRDD 返回，并没有实际运算. 以下算子也是一样123456789101112131415161718192021222324252627282930313233/** * Return a new RDD by first applying a function to all elements of this * RDD, and then flattening the results. */def flatMap[U: ClassTag](f: T =&gt; TraversableOnce[U]): RDD[U] = withScope &#123; val cleanF = sc.clean(f) new MapPartitionsRDD[U, T](this, (context, pid, iter) =&gt; iter.flatMap(cleanF))&#125;/** * Return a new RDD containing only the elements that satisfy a predicate. */def filter(f: T =&gt; Boolean): RDD[T] = withScope &#123; val cleanF = sc.clean(f) new MapPartitionsRDD[T, T]( this, (context, pid, iter) =&gt; iter.filter(cleanF), preservesPartitioning = true)&#125;/** * Return a new RDD containing the distinct elements in this RDD. */def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope &#123; map(x =&gt; (x, null)).reduceByKey((x, y) =&gt; x, numPartitions).map(_._1)&#125;/** * Return a new RDD containing the distinct elements in this RDD. */def distinct(): RDD[T] = withScope &#123; distinct(partitions.length)&#125; 再来看看action12345678910111213141516171819202122232425262728293031/** * Return the number of elements in the RDD. */ def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum/** * Applies a function f to all elements of this RDD. */ def foreach(f: T =&gt; Unit): Unit = withScope &#123; val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) =&gt; iter.foreach(cleanF)) &#125; /** * Applies a function f to each partition of this RDD. */ def foreachPartition(f: Iterator[T] =&gt; Unit): Unit = withScope &#123; val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) =&gt; cleanF(iter)) &#125; /** * Return an array that contains all of the elements in this RDD. * * @note This method should only be used if the resulting array is expected to be small, as * all the data is loaded into the driver's memory. */ def collect(): Array[T] = withScope &#123; val results = sc.runJob(this, (iter: Iterator[T]) =&gt; iter.toArray) Array.concat(results: _*) &#125; 可以看到，action本质上都在调用SparkContext的runJob方法,在RDD的所有partition上运行一个job，返回一个数组，每个元素包含一个partition的结果1234567891011/** * Run a job on all partitions in an RDD and return the results in an array. * * @param rdd target RDD to run tasks on * @param func a function to run on each partition of the RDD * @return in-memory collection with a result of the job (each collection element will contain * a result from one partition) */def runJob[T, U: ClassTag](rdd: RDD[T], func: Iterator[T] =&gt; U): Array[U] = &#123; runJob(rdd, func, 0 until rdd.partitions.length)&#125; SparkContext后面有空再讲 dependencydependency表示RDD之间的依赖关系 RDD的构造函数里有dependency.可通过构造函数，判断是什么依赖 1234abstract class RDD[T: ClassTag]( @transient private var _sc: SparkContext, @transient private var deps: Seq[Dependency[_]] ) 有时，构造函数里的dependency是 Nil ,这时可以通过 getDependencies 方法获得依赖类型 12345678910111213141516171819@DeveloperApiclass UnionRDD[T: ClassTag]( sc: SparkContext, var rdds: Seq[RDD[T]]) extends RDD[T](sc, Nil) &#123; // Nil since we implement getDependencies ... override def getDependencies: Seq[Dependency[_]] = &#123; val deps = new ArrayBuffer[Dependency[_]] var pos = 0 for (rdd &lt;- rdds) &#123; deps += new RangeDependency(rdd, 0, pos, rdd.partitions.length) pos += rdd.partitions.length &#125; deps &#125; ...&#125; dependency类图如下 父RDD : 子RDD possible dependency 1:1 OneToOneDependency N:1 N:1 NarrowDependency N:N N:N NarrowDependency(笛卡尔) ShuffleDependency, 见 ShuffledRDD 有时，RDD会使用匿名内部类，不在上面的类图里面！如 笛卡尔积EDD CartesianRDD123456789101112131415161718192021222324252627282930313233343536373839404142private[spark]class CartesianRDD[T: ClassTag, U: ClassTag]( sc: SparkContext, var rdd1 : RDD[T], var rdd2 : RDD[U]) extends RDD[(T, U)](sc, Nil) with Serializable &#123; val numPartitionsInRdd2 = rdd2.partitions.length override def getPartitions: Array[Partition] = &#123; // create the cross product split val array = new Array[Partition](rdd1.partitions.length * rdd2.partitions.length) for (s1 &lt;- rdd1.partitions; s2 &lt;- rdd2.partitions) &#123; val idx = s1.index * numPartitionsInRdd2 + s2.index array(idx) = new CartesianPartition(idx, rdd1, rdd2, s1.index, s2.index) &#125; array &#125; override def getPreferredLocations(split: Partition): Seq[String] = &#123; val currSplit = split.asInstanceOf[CartesianPartition] (rdd1.preferredLocations(currSplit.s1) ++ rdd2.preferredLocations(currSplit.s2)).distinct &#125; override def compute(split: Partition, context: TaskContext): Iterator[(T, U)] = &#123; val currSplit = split.asInstanceOf[CartesianPartition] for (x &lt;- rdd1.iterator(currSplit.s1, context); y &lt;- rdd2.iterator(currSplit.s2, context)) yield (x, y) &#125; override def getDependencies: Seq[Dependency[_]] = List( new NarrowDependency(rdd1) &#123; def getParents(id: Int): Seq[Int] = List(id / numPartitionsInRdd2) &#125;, new NarrowDependency(rdd2) &#123; def getParents(id: Int): Seq[Int] = List(id % numPartitionsInRdd2) &#125; ) ...&#125; NarrowDependency VS ShuffleDependency =&gt; FullDependency VS PartialDependency NarrowDependency: 子rdd依赖父RDD中固定的Partition ShuffleDependency: 子RDD对父RDD中的所有RDD都可能产生依赖.父RDD中的每个partition分成多个部分transformation到子RDD，reducebyKey等, OneToOneDependency: 1对1，map，mapPartitions等 RangeDependency: 仅用于UnionRDD PruneDependency: PartitionPruningRDD prune意为修剪 例如: RDD通过range分区，DAG在key上有filter，range不覆盖key的分区不用发起任务 123456789101112131415161718192021/** * :: DeveloperApi :: * An RDD used to prune RDD partitions/partitions so we can avoid launching tasks on * all partitions. An example use case: If we know the RDD is partitioned by range, * and the execution DAG has a filter on the key, we can avoid launching tasks * on partitions that don't have the range covering the key. */@DeveloperApiclass PartitionPruningRDD[T: ClassTag]( prev: RDD[T], partitionFilterFunc: Int =&gt; Boolean) extends RDD[T](prev.context, List(new PruneDependency(prev, partitionFilterFunc))) &#123; override def compute(split: Partition, context: TaskContext): Iterator[T] = &#123; firstParent[T].iterator( split.asInstanceOf[PartitionPruningRDDPartition].parentSplit, context) &#125; override protected def getPartitions: Array[Partition] = dependencies.head.asInstanceOf[PruneDependency[T]].partitions&#125; 有时一个transformation算子导致的dependency并不固定，如 PairRDDFunctions.cogroup 1234567891011121314151617181920212223242526272829303132333435363738394041424344class PairRDDFunctions[K, V](self: RDD[(K, V)]) (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null) extends Logging with Serializable &#123; /** * For each key k in `this` or `other1` or `other2` or `other3`, * return a resulting RDD that contains a tuple with the list of values * for that key in `this`, `other1`, `other2` and `other3`. */ def cogroup[W1, W2, W3](other1: RDD[(K, W1)], other2: RDD[(K, W2)], other3: RDD[(K, W3)], partitioner: Partitioner) : RDD[(K, (Iterable[V], Iterable[W1], Iterable[W2], Iterable[W3]))] = self.withScope &#123; if (partitioner.isInstanceOf[HashPartitioner] &amp;&amp; keyClass.isArray) &#123; throw new SparkException("HashPartitioner cannot partition array keys.") &#125; val cg = new CoGroupedRDD[K](Seq(self, other1, other2, other3), partitioner) cg.mapValues &#123; case Array(vs, w1s, w2s, w3s) =&gt; (vs.asInstanceOf[Iterable[V]], w1s.asInstanceOf[Iterable[W1]], w2s.asInstanceOf[Iterable[W2]], w3s.asInstanceOf[Iterable[W3]]) &#125; &#125;&#125;class CoGroupedRDD[K: ClassTag]( @transient var rdds: Seq[RDD[_ &lt;: Product2[K, _]]], part: Partitioner) extends RDD[(K, Array[Iterable[_]])](rdds.head.context, Nil) &#123; override def getDependencies: Seq[Dependency[_]] = &#123; rdds.map &#123; rdd: RDD[_] =&gt; if (rdd.partitioner == Some(part)) &#123; logDebug("Adding one-to-one dependency with " + rdd) new OneToOneDependency(rdd) &#125; else &#123; logDebug("Adding shuffle dependency with " + rdd) new ShuffleDependency[K, Any, CoGroupCombiner]( rdd.asInstanceOf[RDD[_ &lt;: Product2[K, _]]], part, serializer) &#125; &#125; &#125;&#125; 当子RDD的partitioner与父RDD的partitioner相等时，产生OneToOneDependency. 否则，ShuffleDependency ShuffleDependency 左边的 RDD 中的 record 要求是 [K, V] 型的，经过 ShuffleDependency 后，包含相同 key 的 records 会被 aggregate 到一起，然后在 aggregated 的 records 上执行不同的计算逻辑。实际执行时很多 transformation() 如 groupByKey()，reduceByKey() 是边 aggregate 数据边执行计算逻辑的，因此共同之处就是 aggregate 同时 compute()。Spark 使用 combineByKeyWithClassTag 来实现这个 aggregate + compute() 的基础操作。1234567def combineByKeyWithClassTag[C]( createCombiner: V =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C, partitioner: Partitioner, mapSideCombine: Boolean = true, serializer: Serializer = null) stage and task如何划分stage，task? 从 finalRDD 往前，遇到 ShuffleDependency 就切分 Stage,遇 NarrowDependency 就加入该 Stage，每个 Stage 里 Task 的数量，由该 Stage 最后一个 RDD 的 partition 数量决定。如果 Stage 要产生 result，则里面的 Task 都是 ResultTask，否则都是 ShuffleMapTask。 pipeline 思想：数据用的时候再算，而且数据是流到要计算的位置 下图stage中，包含 map union partitionBy join 以笛卡尔积为例一共6个 ResultTask.每个 Task 计算3个 RDD，读2个 data block。计算 CartesianRDD 的 partition，需要从2个RDD获取records，由于存在一个ResultTask里，不需要shuffle 不管是1:1,N:1,N:N，只要是NarrowDependency chain，就可以进行 pipeline CartesianRDD 源码如下,很简单1234567891011121314151617181920212223242526272829303132333435363738394041424344454647private[spark]class CartesianRDD[T: ClassTag, U: ClassTag]( sc: SparkContext, var rdd1 : RDD[T], var rdd2 : RDD[U]) extends RDD[(T, U)](sc, Nil) with Serializable &#123; val numPartitionsInRdd2 = rdd2.partitions.length // m * n override def getPartitions: Array[Partition] = &#123; // create the cross product split val array = new Array[Partition](rdd1.partitions.length * rdd2.partitions.length) for (s1 &lt;- rdd1.partitions; s2 &lt;- rdd2.partitions) &#123; val idx = s1.index * numPartitionsInRdd2 + s2.index array(idx) = new CartesianPartition(idx, rdd1, rdd2, s1.index, s2.index) &#125; array &#125; override def getPreferredLocations(split: Partition): Seq[String] = &#123; val currSplit = split.asInstanceOf[CartesianPartition] (rdd1.preferredLocations(currSplit.s1) ++ rdd2.preferredLocations(currSplit.s2)).distinct &#125; override def compute(split: Partition, context: TaskContext): Iterator[(T, U)] = &#123; val currSplit = split.asInstanceOf[CartesianPartition] for (x &lt;- rdd1.iterator(currSplit.s1, context); y &lt;- rdd2.iterator(currSplit.s2, context)) yield (x, y) &#125; override def getDependencies: Seq[Dependency[_]] = List( new NarrowDependency(rdd1) &#123; def getParents(id: Int): Seq[Int] = List(id / numPartitionsInRdd2) &#125;, new NarrowDependency(rdd2) &#123; def getParents(id: Int): Seq[Int] = List(id % numPartitionsInRdd2) &#125; ) override def clearDependencies() &#123; super.clearDependencies() rdd1 = null rdd2 = null &#125;&#125; 再看看MapPartitionsRDD源码12345678910111213141516171819202122/** * An RDD that applies the provided function to every partition of the parent RDD. */private[spark] class MapPartitionsRDD[U: ClassTag, T: ClassTag]( var prev: RDD[T], f: (TaskContext, Int, Iterator[T]) =&gt; Iterator[U], // (TaskContext, partition index, iterator) preservesPartitioning: Boolean = false) extends RDD[U](prev) &#123; //firstParent 父RDD中的第一个 override val partitioner = if (preservesPartitioning) firstParent[T].partitioner else None override def getPartitions: Array[Partition] = firstParent[T].partitions override def compute(split: Partition, context: TaskContext): Iterator[U] = f(context, split.index, firstParent[T].iterator(split, context)) override def clearDependencies() &#123; super.clearDependencies() prev = null &#125;&#125; 可见，compute调用父RDD的iterator方法，将父RDD或data block中的records一个个fetch过来 SparkInternals JobPhysicalPlan]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>big data</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[learn spark by an example]]></title>
    <url>%2F2019%2F05%2F15%2Flearn-spark-by-an-example%2F</url>
    <content type="text"><![CDATA[spark执行流程中，包括transformation，action两种操作。action切分job，transformation中的shuffle切分stage。 以spark自带examples里的的GroupByTest为例，进行分析 代码： 12345678910111213141516171819202122232425262728293031object GroupByTest &#123; def main(args: Array[String]) &#123; val spark = SparkSession .builder .appName("GroupBy Test") .config(new SparkConf().setMaster("local[2]")) .getOrCreate() val numMappers = if (args.length &gt; 0) args(0).toInt else 2 val numKVPairs = if (args.length &gt; 1) args(1).toInt else 1000 val valSize = if (args.length &gt; 2) args(2).toInt else 1000 val numReducers = if (args.length &gt; 3) args(3).toInt else numMappers val pairs1 = spark.sparkContext.parallelize(0 until numMappers, numMappers).flatMap &#123; p =&gt; val ranGen = new Random val arr1 = new Array[(Int, Array[Byte])](numKVPairs) for (i &lt;- 0 until numKVPairs) &#123; val byteArr = new Array[Byte](valSize) ranGen.nextBytes(byteArr) arr1(i) = (ranGen.nextInt(Int.MaxValue), byteArr) &#125; arr1 &#125;.cache() // Enforce that everything has been calculated and in cache pairs1.count() println(pairs1.groupByKey(numReducers).count()) spark.stop() &#125;&#125; 1bin/spark-submit --master spark://localhost:7077 --jars /opt/spark/examples/jars/spark-examples_2.11-2.3.1.jar --class org.apache.spark.examples.GroupByTest 2 1000 1000 2 运行完后，进入18080端口，查看Jobs如下：可见，被两次action操作——count()切割为两个job， 重点看Job Id 1，job被groupByKey切分为两个stage。stage 1 进行shuffle write，stage 2 进行shuffle wirte 不执行count(), 直接执行println(pairs1.toDebugString),输出12(2) MapPartitionsRDD[1] at flatMap at GroupByTest.scala:40 [Memory Deserialized 1x Replicated] | ParallelCollectionRDD[0] at parallelize at GroupByTest.scala:40 [Memory Deserialized 1x Replicated] 可见输出并不完整（因为没有action） 先执行count，再运行println(pairs1.toDebugString),输出123(2) MapPartitionsRDD[1] at flatMap at GroupByTest.scala:40 [Memory Deserialized 1x Replicated] | CachedPartitions: 2; MemorySize: 2.0 MB; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B | ParallelCollectionRDD[0] at parallelize at GroupByTest.scala:40 [Memory Deserialized 1x Replicated] 输出少ShuffledRDD 将groupByKey后的rdd重新赋值123val groupedRdd = pairs1.groupByKey()println(groupedRdd.count())println(groupedRdd.toDebugString) 输出1234(2) ShuffledRDD[2] at groupByKey at GroupByTest.scala:53 [] +-(2) MapPartitionsRDD[1] at flatMap at GroupByTest.scala:40 [] | CachedPartitions: 2; MemorySize: 2.0 MB; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B | ParallelCollectionRDD[0] at parallelize at GroupByTest.scala:40 [] RDD.toDebugString的作用：A description of this RDD and its recursive dependencies for debugging. 总结：两次count()将application分为两个job， job0由第一个action——count()触发，只包含一个stage.包含 numMappers 个ResultTask。因为执行了cache,会将MapPartitionsRDD缓存到executor的内存如下图： task 执行完后，driver 收集每个 task 的执行结果，然后进行 sum()。job0结束 job1由第二个action——count()触发，包含两个stage(被groupByKey切分). stage1 包含 numMappers 个ShuffleMapTask,进行shuffle write stage2 包含 numReducers 个ResultTask，进行shuffle read，count task 执行完后，driver 收集每个 task 的执行结果，然后进行 sum()。job1结束 参考 SparkInternals 概览]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>big data</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark概览]]></title>
    <url>%2F2019%2F05%2F15%2Fspark-overview%2F</url>
    <content type="text"><![CDATA[简介spark 2.4.3包含 Spark Sql,用于sql与结构化数据处理 MLlib，用于机器学习 GraphX，图处理 Spark Streaming，流处理 架构 SparkContext源码解析可以连接不同集群管理器(standalone,mesos,yarn).一旦连接上，spark会申请集群node上的executor来执行计算、存储数据。然后，发送代码(defined by jar or python files to SparkContext)到executor。最后，sparkContext发送tasks给executors执行 名词解释 Term Meaning Driver program The process running the main() function of the application and creating the SparkContext Deploy mode Distinguishes where the driver process runs. In “cluster” mode, the framework launches the driver inside of the cluster. In “client” mode, the submitter launches the driver outside of the cluster. Executor A process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them. Each application has its own executors. Stage Each job gets divided into smaller sets of tasks called stages that depend on each other (similar to the map and reduce stages in MapReduce); you’ll see this term used in the driver’s logs. Job A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g. save, collect); you’ll see this term used in the driver’s logs. 配置 HistoryServerspark web ui 4040端口在spark跑完后会关掉，配置spark的HistoryServer功能 配置 $SPARK_HOME/conf/spark-defaults.conf1234spark.eventLog.enabled truespark.eventLog.dir hdfs://localhost:9000/spark_logspark.eventLog.compress truespark.history.fs.logDirectory hdfs://localhost:9000/spark_log $SPARK_HOME/conf/spark-env.sh1xport SPARK_HISTORY_OPTS=&quot;-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=10000 -Dspark.history.fs.logDirectory=hdfs://localhost:9000/spark_log&quot; 在hdfs创建log目录1$HADOOP_HOME/bin/hdfs dfs -mkdir /spark_log 1bin/spark-submit --master spark://localhost:7077 --jars /opt/spark/examples/jars/spark-examples_2.11-2.3.1.jar --class org.apache.spark.examples.GroupByTest 2 1000 1000 2 通过spark-submit 提交的application会在http://spark-master:18080/展示 参考： spark cluster overview]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>big data</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F10%2F15%2Fundefined%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
